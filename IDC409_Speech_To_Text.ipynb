{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bf1a27c-27c1-462f-a798-301af54489ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available tiers: ['words', 'phones']\n",
      "Tier 'phonemes' not found in the TextGrid file.\n"
     ]
    }
   ],
   "source": [
    "from textgrid import TextGrid\n",
    "\n",
    "# Load the TextGrid file\n",
    "textgrid_path = \"C:/Users/siddh/Documents/aligned_librispeech/19/19-198-0000.TextGrid\"\n",
    "tg = TextGrid.fromFile(textgrid_path)\n",
    "\n",
    "# Print available tier names to confirm\n",
    "print(\"Available tiers:\", [tier.name for tier in tg.tiers])\n",
    "\n",
    "# Access the correct tier name (e.g., \"phonemes\" or \"words\")\n",
    "tier_name = \"phonemes\"  # Replace with the actual name of your tier\n",
    "tier = tg.getFirst(tier_name)\n",
    "\n",
    "# Check if the tier exists\n",
    "if tier is not None:\n",
    "    # Extract intervals (start time, end time, label) from the tier\n",
    "    alignment_data = []\n",
    "    for interval in tier.intervals:\n",
    "        alignment_data.append({\n",
    "            \"label\": interval.mark,\n",
    "            \"start_time\": interval.minTime,\n",
    "            \"end_time\": interval.maxTime\n",
    "        })\n",
    "\n",
    "    # Display the extracted alignment data\n",
    "    for entry in alignment_data:\n",
    "        print(f\"Label: {entry['label']}, Start: {entry['start_time']}, End: {entry['end_time']}\")\n",
    "else:\n",
    "    print(f\"Tier '{tier_name}' not found in the TextGrid file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "281b6e1b-dcaa-4748-84fb-3b4b377545b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextGrid(None, [IntervalTier(words, [Interval(0.0, 0.57, None), Interval(0.57, 1.25, northanger), Interval(1.25, 1.3, None), Interval(1.3, 1.77, abbey), Interval(1.77, 1.965, None)]), IntervalTier(phones, [Interval(0.0, 0.57, None), Interval(0.57, 0.69, N), Interval(0.69, 0.73, AO2), Interval(0.73, 0.82, R), Interval(0.82, 0.91, TH), Interval(0.91, 1.05, AE1), Interval(1.05, 1.13, NG), Interval(1.13, 1.25, ER0), Interval(1.25, 1.3, None), Interval(1.3, 1.46, AE1), Interval(1.46, 1.54, B), Interval(1.54, 1.77, IY0), Interval(1.77, 1.965, None)])])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textgrid import TextGrid\n",
    "\n",
    "# Load the TextGrid file\n",
    "textgrid_path = \"C:/Users/siddh/Documents/aligned_librispeech/19/19-198-0000.TextGrid\"\n",
    "tg = TextGrid.fromFile(textgrid_path)\n",
    "tg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ff83370-2f19-40c2-a20e-97e704c694d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': None,\n",
       " 'minTime': 0.0,\n",
       " 'maxTime': 1.965,\n",
       " 'tiers': [IntervalTier(words, [Interval(0.0, 0.57, None), Interval(0.57, 1.25, northanger), Interval(1.25, 1.3, None), Interval(1.3, 1.77, abbey), Interval(1.77, 1.965, None)]),\n",
       "  IntervalTier(phones, [Interval(0.0, 0.57, None), Interval(0.57, 0.69, N), Interval(0.69, 0.73, AO2), Interval(0.73, 0.82, R), Interval(0.82, 0.91, TH), Interval(0.91, 1.05, AE1), Interval(1.05, 1.13, NG), Interval(1.13, 1.25, ER0), Interval(1.25, 1.3, None), Interval(1.3, 1.46, AE1), Interval(1.46, 1.54, B), Interval(1.54, 1.77, IY0), Interval(1.77, 1.965, None)])],\n",
       " 'strict': True}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tg.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a39fbf3-1470-4287-8b2e-63251a0aba45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IntervalTier(words, [Interval(0.0, 0.57, None), Interval(0.57, 1.25, northanger), Interval(1.25, 1.3, None), Interval(1.3, 1.77, abbey), Interval(1.77, 1.965, None)])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tg.tiers\n",
    "tg.tiers[0]\n",
    "tg.tiers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13c382f3-778a-4cbc-ae5f-c0a2879d03cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from textgrid import TextGrid\n",
    "\n",
    "# Function to parse TextGrid files and store data in a dictionary\n",
    "def parse_textgrid_to_dict(textgrid_path):\n",
    "    # Initialize dictionary to hold alignment data for the file\n",
    "    alignment_data = {}\n",
    "\n",
    "    # Load the TextGrid file\n",
    "    tg = TextGrid.fromFile(textgrid_path)\n",
    "    \n",
    "    # Define the tiers you want to parse\n",
    "    tier_names = [\"phones\", \"words\"]  # Replace with your TextGrid's actual tier names\n",
    "\n",
    "    for tier_name in tier_names:\n",
    "        # Try to get the specified tier, skip if it doesn't exist\n",
    "        tier = tg.getFirst(tier_name)\n",
    "        if tier is None:\n",
    "            #print(f\"Tier '{tier_name}' not found in {textgrid_path}. Skipping this tier.\")\n",
    "            continue\n",
    "\n",
    "        # Initialize a list to store intervals for this tier\n",
    "        tier_data = []\n",
    "        for interval in tier.intervals:\n",
    "            # Append interval data to the list, using \"N/A\" for empty labels\n",
    "            tier_data.append({\n",
    "                \"label\": interval.mark if interval.mark else \"N/A\",\n",
    "                \"start_time\": interval.minTime,\n",
    "                \"end_time\": interval.maxTime\n",
    "            })\n",
    "        \n",
    "        # Add tier data to alignment data\n",
    "        alignment_data[tier_name] = tier_data\n",
    "    \n",
    "    return alignment_data\n",
    "\n",
    "# Main function to parse all TextGrids in the directory structure and store in a dictionary\n",
    "def parse_all_textgrids_to_dict(base_directory):\n",
    "    all_alignments = {}\n",
    "    \n",
    "    # Walk through all folders and files\n",
    "    for root, dirs, files in os.walk(base_directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".TextGrid\"):\n",
    "                textgrid_path = os.path.join(root, file)\n",
    "                #print(f\"Parsing {textgrid_path}\")\n",
    "\n",
    "                # Parse the TextGrid file and add it to the dictionary\n",
    "                alignment_data = parse_textgrid_to_dict(textgrid_path)\n",
    "                all_alignments[textgrid_path] = alignment_data\n",
    "\n",
    "    return all_alignments\n",
    "\n",
    "# Specify the base directory containing your folders and TextGrid files\n",
    "base_directory = \"C:/Users/siddh/Documents/aligned_librispeech\"\n",
    "all_alignments = parse_all_textgrids_to_dict(base_directory)\n",
    "\n",
    "# Display parsed data for verification\n",
    "for textgrid_path, alignment_data in all_alignments.items():\n",
    "    #print(f\"\\nParsed data for {textgrid_path}:\")\n",
    "    pass\n",
    "    for tier, intervals in alignment_data.items():\n",
    "        #print(f\"  Tier: {tier}\")\n",
    "        pass\n",
    "        for interval in intervals:\n",
    "            pass\n",
    "            #print(f\"    Label: {interval['label']}, Start: {interval['start_time']}, End: {interval['end_time']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69cc71ef-5e2c-4a58-82b9-3e57dfb8de3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_alignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7585c0e-e4f0-41d7-beb0-12eb7d52f641",
   "metadata": {},
   "outputs": [],
   "source": [
    "#formatted_alignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2d52f13-0db5-4f18-b0c0-f16c5f1262b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def convert_alignments_from_textgrids(all_alignments, base_audio_dir=\"C:/Users/siddh/Downloads/Librispeech/Librispeech\"):\n",
    "    # Dictionary to store processed alignments and corresponding audio path\n",
    "    formatted_alignments = {}\n",
    "\n",
    "    for textgrid_path, tiers_data in all_alignments.items():\n",
    "        # Extract \"phones\" tier data if available\n",
    "        phoneme_intervals = [\n",
    "            (interval[\"start_time\"], interval[\"end_time\"], interval[\"label\"])\n",
    "            for interval in tiers_data.get(\"phones\", [])\n",
    "            if interval[\"label\"] != \"N/A\"  # Ignore unmarked intervals\n",
    "        ]\n",
    "        \n",
    "        # Extract \"words\" tier data if available\n",
    "        word_intervals = [\n",
    "            (interval[\"start_time\"], interval[\"end_time\"], interval[\"label\"])\n",
    "            for interval in tiers_data.get(\"words\", [])\n",
    "            if interval[\"label\"] != \"N/A\"  # Ignore unmarked intervals\n",
    "        ]\n",
    "        \n",
    "        # Derive the audio path based on TextGrid path identifier\n",
    "        identifier = os.path.basename(textgrid_path).replace(\".TextGrid\", \"\")\n",
    "        audio_file_path = os.path.join(base_audio_dir, identifier[:2], f\"{identifier}.wav\")\n",
    "\n",
    "        # Check if the corresponding audio file exists\n",
    "        if not os.path.exists(audio_file_path):\n",
    "            print(f\"Audio file not found for {identifier}: {audio_file_path}\")\n",
    "            continue\n",
    "        \n",
    "        # Store phoneme intervals, word intervals, and audio path in the dictionary\n",
    "        formatted_alignments[textgrid_path] = {\n",
    "            \"audio_path\": audio_file_path,\n",
    "            \"phoneme_intervals\": phoneme_intervals,\n",
    "            \"word_intervals\": word_intervals\n",
    "        }\n",
    "    \n",
    "    return formatted_alignments\n",
    "\n",
    "# Example usage\n",
    "formatted_alignments = convert_alignments_from_textgrids(all_alignments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "797943fa-2325-4aa3-abcc-4c0234b60673",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'audio_path': 'C:/Users/siddh/Downloads/Librispeech/Librispeech\\\\19\\\\19-198-0000.wav',\n",
       " 'phoneme_intervals': [(0.57, 0.69, 'N'),\n",
       "  (0.69, 0.73, 'AO2'),\n",
       "  (0.73, 0.82, 'R'),\n",
       "  (0.82, 0.91, 'TH'),\n",
       "  (0.91, 1.05, 'AE1'),\n",
       "  (1.05, 1.13, 'NG'),\n",
       "  (1.13, 1.25, 'ER0'),\n",
       "  (1.3, 1.46, 'AE1'),\n",
       "  (1.46, 1.54, 'B'),\n",
       "  (1.54, 1.77, 'IY0')],\n",
       " 'word_intervals': [(0.57, 1.25, 'northanger'), (1.3, 1.77, 'abbey')]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(formatted_alignments.values())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2fc4fb8-b0f1-4d57-a9d2-63b15160916a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "def get_mfcc_with_labels(formatted_alignments, sr=16000, mfcc_dim=13):\n",
    "    phoneme_data_with_labels = []\n",
    "    word_data_with_labels = []\n",
    "\n",
    "    for entry in formatted_alignments.values():\n",
    "        # Check for audio path\n",
    "        if \"audio_path\" not in entry:\n",
    "            raise KeyError(\"Each entry in formatted_alignments must contain 'audio_path' to locate audio files.\")\n",
    "        \n",
    "        audio_path = entry[\"audio_path\"]\n",
    "        phoneme_intervals = entry[\"phoneme_intervals\"]\n",
    "        word_intervals = entry[\"word_intervals\"]  # Assuming word-level intervals are included\n",
    "\n",
    "        # Load audio file\n",
    "        audio, _ = librosa.load(audio_path, sr=sr)\n",
    "\n",
    "        # Extract MFCC features (shape: (num_frames, mfcc_dim))\n",
    "        mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=mfcc_dim).T  # Transpose to shape (num_frames, mfcc_dim)\n",
    "\n",
    "        # Calculate frame duration in seconds\n",
    "        frame_duration = len(audio) / len(mfcc) / sr  # Duration of each MFCC frame\n",
    "\n",
    "        # Initialize lists for phoneme and word labels\n",
    "        mfcc_phoneme_labels = []\n",
    "        mfcc_word_labels = []\n",
    "        \n",
    "        for i, mfcc_frame in enumerate(mfcc):\n",
    "            frame_start_time = i * frame_duration\n",
    "\n",
    "            # Find the phoneme label based on frame start time\n",
    "            phoneme_label = next((label for start, end, label in phoneme_intervals if start <= frame_start_time < end), None)\n",
    "            if phoneme_label is not None:\n",
    "                mfcc_phoneme_labels.append((mfcc_frame, phoneme_label))\n",
    "\n",
    "            # Find the word label based on frame start time\n",
    "            word_label = next((label for start, end, label in word_intervals if start <= frame_start_time < end), None)\n",
    "            if word_label is not None:\n",
    "                mfcc_word_labels.append((mfcc_frame, word_label))\n",
    "\n",
    "        # Append data to respective lists\n",
    "        phoneme_data_with_labels.append({\n",
    "            \"audio_path\": audio_path,\n",
    "            \"mfcc_with_labels\": mfcc_phoneme_labels\n",
    "        })\n",
    "        \n",
    "        word_data_with_labels.append({\n",
    "            \"audio_path\": audio_path,\n",
    "            \"mfcc_with_labels\": mfcc_word_labels\n",
    "        })\n",
    "\n",
    "    return phoneme_data_with_labels, word_data_with_labels\n",
    "\n",
    "# Example usage\n",
    "mfcc_phoneme_data, mfcc_word_data = get_mfcc_with_labels(formatted_alignments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "967215d1-c693-4cc4-b17f-386a2f6a1cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mfcc_phoneme_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78536383-c0cd-4773-9fb8-e8f0fb8be056",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mfcc_word_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4bcdf115-4693-4b13-8f5f-d25c2cdea3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if this is even doing anything\n",
    "def extract_unique_phonemes_and_words(formatted_alignments):\n",
    "    unique_phonemes = set()\n",
    "    unique_words = set()\n",
    "\n",
    "    # Iterate over each alignment entry\n",
    "    for alignment in formatted_alignments.values():\n",
    "        # Extract phoneme labels and add them to the phoneme set\n",
    "        for _, _, phoneme in alignment.get(\"phoneme_intervals\", []):\n",
    "            unique_phonemes.add(phoneme)\n",
    "\n",
    "        # Extract word labels and add them to the word set\n",
    "        for _, _, word in alignment.get(\"word_intervals\", []):\n",
    "            unique_words.add(word)\n",
    "\n",
    "    # Return both unique phonemes and unique words as sorted lists\n",
    "    return sorted(unique_phonemes), sorted(unique_words)\n",
    "\n",
    "# Usage\n",
    "unique_phonemes, unique_words = extract_unique_phonemes_and_words(formatted_alignments)\n",
    "#print(f\"Unique phonemes: {unique_phonemes}\")\n",
    "#print(f\"Unique words: {unique_words}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3089b18-61dc-434d-ab8f-49eae6a92890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68\n"
     ]
    }
   ],
   "source": [
    "print(len(unique_phonemes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5104bec4-8e11-4fe4-b2d6-edecb9075637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2356\n"
     ]
    }
   ],
   "source": [
    "print(len(unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e4ad44a-b189-4ea3-9b35-719aaaf5e1b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs shape: torch.Size([69165, 100, 13])\n",
      "Targets (phoneme) shape: torch.Size([69165, 100])\n",
      "Targets (word) shape: torch.Size([69165, 100])\n",
      "Unique phoneme classes: ['AA0' 'AA1' 'AA2' 'AE0' 'AE1' 'AE2' 'AH0' 'AH1' 'AH2' 'AO0' 'AO1' 'AO2'\n",
      " 'AW1' 'AW2' 'AY0' 'AY1' 'AY2' 'B' 'CH' 'D' 'DH' 'EH0' 'EH1' 'EH2' 'ER0'\n",
      " 'ER1' 'ER2' 'EY0' 'EY1' 'EY2' 'F' 'G' 'HH' 'IH0' 'IH1' 'IH2' 'IY0' 'IY1'\n",
      " 'IY2' 'JH' 'K' 'L' 'M' 'N' 'NG' 'OW0' 'OW1' 'OW2' 'OY0' 'OY1' 'P' 'R' 'S'\n",
      " 'SH' 'T' 'TH' 'UH0' 'UH1' 'UH2' 'UW0' 'UW1' 'UW2' 'V' 'W' 'Y' 'Z' 'ZH'\n",
      " 'spn']\n",
      "Unique word classes: ['a' 'abated' 'abatement' ... 'zee' 'zero' 'zuyder']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def prepare_data(mfcc_phoneme_data, mfcc_word_data, sequence_length):\n",
    "    \"\"\"\n",
    "    Prepares data for training by extracting MFCC, phoneme, and word labels,\n",
    "    encoding labels, and creating sequences of specified length.\n",
    "\n",
    "    Args:\n",
    "        mfcc_phoneme_data (list): List of dictionaries with MFCC and phoneme labels.\n",
    "        mfcc_word_data (list): List of dictionaries with MFCC and word labels.\n",
    "        sequence_length (int): Length of the sequences to create.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array of MFCC input sequences.\n",
    "        np.ndarray: Array of encoded phoneme target sequences.\n",
    "        np.ndarray: Array of encoded word target sequences.\n",
    "        LabelEncoder: Encoder for phoneme labels.\n",
    "        LabelEncoder: Encoder for word labels.\n",
    "    \"\"\"\n",
    "    # Lists to hold input-output pairs\n",
    "    inputs = []\n",
    "    targets_phoneme = []\n",
    "    targets_word = []\n",
    "\n",
    "    # Create lists to gather all phoneme and word labels for fitting the encoder\n",
    "    all_phoneme_labels = []\n",
    "    all_word_labels = []\n",
    "\n",
    "    # Extract phoneme labels from mfcc_phoneme_data\n",
    "    for entry in mfcc_phoneme_data:\n",
    "        mfcc_with_labels = entry[\"mfcc_with_labels\"]\n",
    "        phoneme_labels = [label for _, label in mfcc_with_labels]\n",
    "        all_phoneme_labels.extend(phoneme_labels)\n",
    "\n",
    "    # Extract word labels from mfcc_word_data\n",
    "    for entry in mfcc_word_data:\n",
    "        word_intervals = entry[\"mfcc_with_labels\"]\n",
    "        word_labels = [word for _, word in word_intervals]\n",
    "        all_word_labels.extend(word_labels)\n",
    "\n",
    "    # Fit label encoders on both phoneme and word labels\n",
    "    phoneme_label_encoder = LabelEncoder()\n",
    "    word_label_encoder = LabelEncoder()\n",
    "    phoneme_label_encoder.fit(all_phoneme_labels)\n",
    "    word_label_encoder.fit(all_word_labels)\n",
    "\n",
    "    # Iterate through the phoneme data to create sequences\n",
    "    for entry in mfcc_phoneme_data:\n",
    "        mfcc_with_labels = entry[\"mfcc_with_labels\"]\n",
    "        mfcc_array = np.array([mfcc for mfcc, _ in mfcc_with_labels])  # Convert MFCC features to array\n",
    "        phoneme_labels = [label for _, label in mfcc_with_labels]\n",
    "        encoded_phoneme_labels = phoneme_label_encoder.transform(phoneme_labels)\n",
    "\n",
    "        for start in range(len(mfcc_array) - sequence_length + 1):\n",
    "            end = start + sequence_length\n",
    "            inputs.append(mfcc_array[start:end])\n",
    "            targets_phoneme.append(encoded_phoneme_labels[start:end])\n",
    "\n",
    "    # Iterate through the word data to create sequences\n",
    "    for entry in mfcc_word_data:\n",
    "        word_intervals = entry[\"mfcc_with_labels\"]\n",
    "        word_labels = [word for _, word in word_intervals]\n",
    "        encoded_word_labels = word_label_encoder.transform(word_labels)\n",
    "\n",
    "        for start in range(len(encoded_word_labels) - sequence_length + 1):\n",
    "            end = start + sequence_length\n",
    "            targets_word.append(encoded_word_labels[start:end])\n",
    "\n",
    "    # Return inputs, targets (phoneme and word), and label encoders\n",
    "    return np.array(inputs), np.array(targets_phoneme), np.array(targets_word), phoneme_label_encoder, word_label_encoder\n",
    "\n",
    "# Example usage\n",
    "sequence_length = 100  # Define your desired sequence length\n",
    "inputs, targets_phoneme, targets_word, phoneme_label_encoder, word_label_encoder = prepare_data(\n",
    "    mfcc_phoneme_data, mfcc_word_data, sequence_length\n",
    ")\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "inputs_tensor = torch.tensor(inputs, dtype=torch.float32)  # Shape: (num_sequences, sequence_length, num_mfcc_features)\n",
    "targets_phoneme_tensor = torch.tensor(targets_phoneme, dtype=torch.long)  # Shape: (num_sequences, sequence_length)\n",
    "targets_word_tensor = torch.tensor(targets_word, dtype=torch.long)  # Shape: (num_sequences, sequence_length)\n",
    "\n",
    "# Check the shapes\n",
    "print(f\"Inputs shape: {inputs_tensor.shape}\")\n",
    "print(f\"Targets (phoneme) shape: {targets_phoneme_tensor.shape}\")\n",
    "print(f\"Targets (word) shape: {targets_word_tensor.shape}\")\n",
    "print(f\"Unique phoneme classes: {phoneme_label_encoder.classes_}\")\n",
    "print(f\"Unique word classes: {word_label_encoder.classes_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "83fb97f5-7f73-470c-b6de-35f05ebcaa34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 4.220840930938721\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class MFCCtoPhonemeLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=1):\n",
    "        super(MFCCtoPhonemeLSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Define the LSTM layer\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "\n",
    "        # Define the output layer that maps hidden state to phoneme classes\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden and cell state\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
    "\n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "\n",
    "        # Reshape out for the fully connected layer\n",
    "        out = out.reshape(-1, self.hidden_dim)\n",
    "\n",
    "        # Pass through the fully connected layer and return\n",
    "        out = self.fc(out)\n",
    "        out = out.view(x.size(0), -1, out.size(-1))  # reshape to (batch_size, seq_len, output_dim)\n",
    "        return out\n",
    "\n",
    "# Parameters\n",
    "input_dim = 13  # Number of MFCC features per frame\n",
    "hidden_dim = 128  # Size of the hidden layer in LSTM\n",
    "output_dim = phoneme_label_encoder.classes_.size  # Number of phoneme classes (or 61 if using the full set)\n",
    "num_layers = 2  # Number of LSTM layers\n",
    "\n",
    "# Instantiate the model, loss function, and optimizer\n",
    "MFCC_model = MFCCtoPhonemeLSTM(input_dim, hidden_dim, output_dim, num_layers)\n",
    "criterion = nn.CrossEntropyLoss()  # Loss function\n",
    "optimizer = optim.Adam(MFCC_model.parameters(), lr=0.001)\n",
    "\n",
    "# Example dummy input (batch_size, sequence_length, input_dim)\n",
    "batch_size = 16\n",
    "sequence_length = 100  # Number of frames in each sequence\n",
    "x = torch.randn(batch_size, sequence_length, input_dim)  # Random batch of MFCC features\n",
    "y = torch.randint(0, output_dim, (batch_size, sequence_length))  # Random phoneme labels\n",
    "\n",
    "# Forward pass\n",
    "output = MFCC_model(x)  # Output shape: (batch_size, seq_len, output_dim)\n",
    "\n",
    "# Calculate loss (we flatten the output and target to (batch_size * seq_len, output_dim) for cross-entropy)\n",
    "loss = criterion(output.view(-1, output_dim), y.view(-1))\n",
    "print(f\"Loss: {loss.item()}\")\n",
    "\n",
    "# Backward pass and optimize\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2192861-16db-4195-a00a-bc39090c9436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted phonemes: ['L' 'AA0' 'L' 'JH' 'Z' 'K' 'K' 'AA0' 'Z' 'L' 'L' 'JH' 'JH' 'L' 'L' 'L'\n",
      " 'L' 'L' 'EY1' 'L' 'L' 'L' 'JH' 'L' 'L' 'L' 'L' 'L' 'L' 'JH' 'JH' 'JH' 'K'\n",
      " 'K' 'K' 'K' 'L' 'L' 'L' 'L' 'L' 'L' 'K' 'AA0' 'AA0' 'AA0' 'AA0' 'AA0'\n",
      " 'AA0' 'L' 'Z' 'JH' 'L' 'L' 'L' 'L' 'Z' 'Z' 'Z' 'Z' 'Z' 'Z' 'L' 'L' 'L'\n",
      " 'L' 'Z' 'JH' 'Z' 'Z' 'Z' 'Z' 'Z' 'AA0' 'JH' 'Z' 'Z' 'Z' 'Z' 'L' 'L' 'L'\n",
      " 'L' 'IH0' 'L' 'L' 'L' 'L' 'L' 'L' 'Z' 'L' 'L' 'L' 'L' 'L' 'Z' 'Z' 'Z' 'Z']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Assuming model and label_encoder are already defined\n",
    "\n",
    "def convert_indices_to_phonemes(predicted_indices, label_encoder):\n",
    "    \"\"\"\n",
    "    Converts model output (indices) into phoneme labels.\n",
    "    \n",
    "    predicted_indices: Tensor of shape (batch_size, seq_len) containing predicted phoneme indices\n",
    "    label_encoder: The LabelEncoder that maps phoneme indices to phoneme labels\n",
    "    \"\"\"\n",
    "    # Convert indices to phoneme labels\n",
    "    phoneme_labels = phoneme_label_encoder.inverse_transform(predicted_indices.cpu().numpy().flatten())\n",
    "    \n",
    "    # Reshape back to match the sequence structure (batch_size, seq_len)\n",
    "    phoneme_labels = phoneme_labels.reshape(predicted_indices.shape)\n",
    "    \n",
    "    return phoneme_labels\n",
    "\n",
    "# Example forward pass through the model\n",
    "# Assuming `inputs_tensor` contains your MFCC input sequences\n",
    "# (batch_size, seq_len, num_mfcc_features)\n",
    "model = MFCCtoPhonemeLSTM(input_dim=13, hidden_dim=64, output_dim=len(phoneme_label_encoder.classes_), num_layers=1)\n",
    "#inputs_tensor = inputs_tensor[:batch_size]  # Adjust this to match your data\n",
    "#Decoding_Entire Output at once takes a lot of memory\n",
    "Input_tensor_portion = inputs_tensor[:batch_size]\n",
    "predicted_indices = model(Input_tensor_portion)  # Output shape: (batch_size, seq_len, num_phonemes)\n",
    "\n",
    "# Apply softmax to get probabilities (optional, if you want confidence scores)\n",
    "softmax = torch.nn.Softmax(dim=-1)\n",
    "probabilities = softmax(predicted_indices)\n",
    "\n",
    "# Get the predicted phoneme indices (max probability per frame)\n",
    "predicted_phoneme_indices = torch.argmax(probabilities, dim=-1)  # Shape: (batch_size, seq_len)\n",
    "\n",
    "# Convert the predicted indices to phoneme labels\n",
    "predicted_phonemes = convert_indices_to_phonemes(predicted_phoneme_indices, phoneme_label_encoder)\n",
    "\n",
    "# Print the phonemes for the first example (for debugging purposes)\n",
    "print(f\"Predicted phonemes: {predicted_phonemes[0]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b4ad562-5d34-4d3f-a0ff-9d5fa3a10e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split, DataLoader\n",
    "import torch\n",
    "\n",
    "# Define the full dataset\n",
    "full_dataset = torch.utils.data.TensorDataset(inputs_tensor, targets_phoneme_tensor)\n",
    "\n",
    "# Define the split sizes (e.g., 80% train, 20% test)\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n",
    "\n",
    "# Create DataLoaders for both training and testing sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb69ffc4-8008-453c-b362-7432f9a66515",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "\n",
    "num_epochs = 10\n",
    "batch_size = 16\n",
    "\n",
    "# Create batchDataLoader for batching\n",
    "#train_dataset = torch.utils.data.TensorDataset(inputs_tensor, targets_phoneme_tensor)\n",
    "#train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=_size, shuffle=True)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    all_targets = []\n",
    "    all_predictions = []\n",
    "    all_probabilities = []\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    for batch_inputs, batch_targets in train_loader:\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        output = MFCC_model(batch_inputs)  # Forward pass\n",
    "        \n",
    "        # Reshape for loss calculation\n",
    "        output_reshaped = output.view(-1, output_dim)\n",
    "        batch_targets_reshaped = batch_targets.view(-1)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(output_reshaped, batch_targets_reshaped)\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Get predicted classes and probabilities for metrics\n",
    "        predictions = output.argmax(dim=-1)  # Class predictions\n",
    "        probabilities = F.softmax(output, dim=-1)  # Probability scores\n",
    "        \n",
    "        # Flatten and accumulate for F1 and AUC computation\n",
    "        all_targets.extend(batch_targets_reshaped.cpu().numpy())\n",
    "        all_predictions.extend(predictions.view(-1).cpu().numpy())\n",
    "        all_probabilities.extend(probabilities.view(-1, output_dim).detach().cpu().numpy())\n",
    "    \n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(all_targets, all_predictions, average=\"weighted\")\n",
    "    \n",
    "    # Calculate AUC (if binary or multilabel; modify as needed for multiclass)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_targets, all_probabilities, multi_class=\"ovr\", average=\"weighted\")\n",
    "    except ValueError:\n",
    "        auc = None  # AUC may not be available for some cases\n",
    "        \n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss / len(train_loader)}, F1 Score: {f1}, AUC: {auc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d212e46-0c73-49f3-bdd5-b51727c771c9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'F' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 30\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Get predicted classes and probabilities for metrics\u001b[39;00m\n\u001b[0;32m     29\u001b[0m predictions \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Class predictions\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m probabilities \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241m.\u001b[39msoftmax(output, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Probability scores\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Flatten and accumulate for F1 and AUC computation\u001b[39;00m\n\u001b[0;32m     33\u001b[0m all_targets\u001b[38;5;241m.\u001b[39mextend(batch_targets_reshaped\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "\u001b[1;31mNameError\u001b[0m: name 'F' is not defined"
     ]
    }
   ],
   "source": [
    "# Assuming test data tensors and DataLoader are set up\n",
    "# Replace inputs_tensor and targets_phoneme_tensor with your test data\n",
    "#test_inputs_tensor = inputs_tensor\n",
    "#test_dataset = train_dataset\n",
    "#test_targets_tensor = targets_phoneme_tensor\n",
    "#Replace above 2 lines of code\n",
    "#test_dataset = torch.utils.data.TensorDataset(test_inputs_tensor, test_targets_tensor)\n",
    "#test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Testing loop\n",
    "with torch.no_grad():  # Disable gradient calculation for testing\n",
    "    all_targets = []\n",
    "    all_predictions = []\n",
    "    all_probabilities = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch_inputs, batch_targets in test_loader:\n",
    "        output = MFCC_model(batch_inputs)  # Forward pass\n",
    "        \n",
    "        # Reshape for loss calculation\n",
    "        output_reshaped = output.view(-1, output_dim)\n",
    "        batch_targets_reshaped = batch_targets.view(-1)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(output_reshaped, batch_targets_reshaped)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Get predicted classes and probabilities for metrics\n",
    "        predictions = output.argmax(dim=-1)  # Class predictions\n",
    "        probabilities = F.softmax(output, dim=-1)  # Probability scores\n",
    "        \n",
    "        # Flatten and accumulate for F1 and AUC computation\n",
    "        all_targets.extend(batch_targets_reshaped.cpu().numpy())\n",
    "        all_predictions.extend(predictions.view(-1).cpu().numpy())\n",
    "        all_probabilities.extend(probabilities.view(-1, output_dim).detach().cpu().numpy())\n",
    "    \n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(all_targets, all_predictions, average=\"weighted\")\n",
    "    \n",
    "    # Calculate AUC (if binary or multilabel; modify as needed for multiclass)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_targets, all_probabilities, multi_class=\"ovr\", average=\"weighted\")\n",
    "    except ValueError:\n",
    "        auc = None  # AUC may not be available for some cases\n",
    "\n",
    "    print(f\"Test Loss: {total_loss / len(test_loader)}, F1 Score: {f1}, AUC: {auc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "75daaa7f-fa7d-4530-80f1-f1d6449a5527",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from textgrid import TextGrid\n",
    "\n",
    "def parse_textgrid_phoneme_word(textgrid_path):\n",
    "    # Initialize a list to hold phoneme-word mappings\n",
    "    phoneme_word_mappings = []\n",
    "\n",
    "    # Load the TextGrid file\n",
    "    tg = TextGrid.fromFile(textgrid_path)\n",
    "\n",
    "    # Define the tiers you want to parse\n",
    "    tier_names = [\"phones\", \"words\"]\n",
    "\n",
    "    # Get intervals from phones and words tiers\n",
    "    phoneme_intervals = []\n",
    "    word_intervals = []\n",
    "\n",
    "    for tier_name in tier_names:\n",
    "        tier = tg.getFirst(tier_name)\n",
    "        if tier is None:\n",
    "            print(f\"Tier '{tier_name}' not found in {textgrid_path}.\")\n",
    "            return phoneme_word_mappings  # Return empty if any tier is missing\n",
    "\n",
    "        if tier_name == \"phones\":\n",
    "            phoneme_intervals = [(interval.minTime, interval.maxTime, interval.mark) for interval in tier.intervals if interval.mark]\n",
    "        elif tier_name == \"words\":\n",
    "            word_intervals = [(interval.minTime, interval.maxTime, interval.mark) for interval in tier.intervals if interval.mark]\n",
    "\n",
    "    # Create phoneme-word mapping based on overlapping intervals\n",
    "    for phoneme_start, phoneme_end, phoneme in phoneme_intervals:\n",
    "        for word_start, word_end, word in word_intervals:\n",
    "            if phoneme_start >= word_start and phoneme_end <= word_end:\n",
    "                phoneme_word_mappings.append((phoneme, word))\n",
    "                break  # No need to check further once a word is found\n",
    "\n",
    "    return phoneme_word_mappings\n",
    "\n",
    "def parse_all_textgrids_for_mappings(base_directory):\n",
    "    all_mappings = {}\n",
    "    \n",
    "    # Walk through all folders and files in the given base directory\n",
    "    for root, dirs, files in os.walk(base_directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".TextGrid\"):\n",
    "                textgrid_path = os.path.join(root, file)\n",
    "                #print(f\"Parsing {textgrid_path}\")\n",
    "\n",
    "                # Parse the TextGrid file and get phoneme-word mappings\n",
    "                mappings = parse_textgrid_phoneme_word(textgrid_path)\n",
    "                all_mappings[textgrid_path] = mappings\n",
    "\n",
    "    return all_mappings\n",
    "\n",
    "# Example usage\n",
    "base_directory = \"C:/Users/siddh/Documents/aligned_librispeech\"  # Correct assignment of base directory\n",
    "phoneme_word_mappings = parse_all_textgrids_for_mappings(base_directory)\n",
    "\n",
    "# Print or save the mappings as needed\n",
    "for tg_path, mappings in phoneme_word_mappings.items():\n",
    "    #print(f\"Mappings for {tg_path}:\")\n",
    "    pass\n",
    "    for phoneme, word in mappings:\n",
    "        #print(f\"{phoneme} -> {word}\")\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c88729ea-b30d-4245-8faf-c8974605b544",
   "metadata": {},
   "outputs": [],
   "source": [
    "#phoneme_word_mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8a185437-8497-47fa-a881-abef85c1e1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize sets to store unique phonemes and words\n",
    "unique_phonemes = set()\n",
    "unique_words = set()\n",
    "\n",
    "# Loop through all mappings in the dataset\n",
    "for path, mapping in phoneme_word_mappings.items():\n",
    "    for phoneme, word in mapping:\n",
    "        unique_phonemes.add(phoneme)  # Add phoneme to set\n",
    "        unique_words.add(word)  # Add whole word to set\n",
    "\n",
    "# Convert sets to sorted lists for consistent ordering\n",
    "all_phonemes = sorted(unique_phonemes)\n",
    "all_words = sorted(unique_words)\n",
    "\n",
    "# Create word to index mapping\n",
    "word_to_index = {word: idx + 1 for idx, word in enumerate(all_words)}  # Start from 1\n",
    "word_to_index['<pad>'] = 0  # Add padding index if needed\n",
    "\n",
    "# Create index to word mapping for easy lookup later\n",
    "index_to_word = {idx: word for word, idx in word_to_index.items()}\n",
    "\n",
    "\n",
    "\n",
    "#print(\"All unique phonemes:\", all_phonemes)\n",
    "#print(\"All unique characters:\", all_words)\n",
    "#print(\"\\nCharacter to Index Mapping:\", word_to_index)\n",
    "#print(\"\\nIndex to Character Mapping:\", index_to_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "74215bf1-7ec8-4475-9b8c-e8b5b783e511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 9, 2356])\n",
      "Loss: 7.761223793029785\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the Encoder with LSTM or GRU layers\n",
    "class PhonemeEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, hidden_dim, num_layers=1):\n",
    "        super(PhonemeEncoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embed_dim)\n",
    "        self.rnn = nn.LSTM(embed_dim, hidden_dim, num_layers, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)  # Convert phonemes to embedding vectors\n",
    "        outputs, hidden = self.rnn(embedded)  # Forward through RNN\n",
    "        return outputs, hidden  # Return RNN outputs and hidden states\n",
    "\n",
    "# Define the Attention layer to focus on relevant encoder outputs\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attn = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.v = nn.Linear(hidden_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        seq_len = encoder_outputs.size(1)\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, seq_len, 1)  # (batch_size, seq_len, hidden_dim)\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
    "        attention = self.v(energy).squeeze(2)  # (batch_size, seq_len)\n",
    "        return torch.softmax(attention, dim=1)\n",
    "\n",
    "# Define the Decoder with Attention mechanism\n",
    "class WordDecoder(nn.Module):\n",
    "    def __init__(self, output_dim, embed_dim, hidden_dim, num_layers=1):\n",
    "        super(WordDecoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_dim, embed_dim)\n",
    "        self.rnn = nn.LSTM(embed_dim + hidden_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.attention = Attention(hidden_dim)\n",
    "\n",
    "    def forward(self, x, hidden, encoder_outputs):\n",
    "        x = x.unsqueeze(1)  # Add a time dimension\n",
    "        embedded = self.embedding(x)  # Convert characters/words to embeddings\n",
    "\n",
    "        attn_weights = self.attention(hidden[0][-1], encoder_outputs)  # Compute attention weights\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs)  # Apply weights to encoder outputs\n",
    "\n",
    "        rnn_input = torch.cat((embedded, attn_applied), dim=2)  # Combine with embeddings for RNN input\n",
    "        output, hidden = self.rnn(rnn_input, hidden)  # Forward through RNN\n",
    "\n",
    "        output = torch.cat((output.squeeze(1), attn_applied.squeeze(1)), dim=1)  # Concatenate for FC layer\n",
    "        output = self.fc(output)  # Fully connected layer for final character prediction\n",
    "        return output, hidden\n",
    "\n",
    "# Define the Seq2Seq model\n",
    "class PhonemeToWordModel(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(PhonemeToWordModel, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        encoder_outputs, hidden = self.encoder(src)\n",
    "        outputs = []\n",
    "        decoder_input = trg[:, 0]  # Start token for decoder\n",
    "\n",
    "        for t in range(1, trg.size(1)):\n",
    "            output, hidden = self.decoder(decoder_input, hidden, encoder_outputs)\n",
    "            outputs.append(output.unsqueeze(1))\n",
    "            decoder_input = output.argmax(1)  # Use predicted character as input in the next step\n",
    "\n",
    "        return torch.cat(outputs, dim=1)\n",
    "\n",
    "# Model hyperparameters\n",
    "input_dim = len(all_phonemes)\n",
    "output_dim = len(all_words)\n",
    "embed_dim = 64\n",
    "hidden_dim = 128\n",
    "num_layers = 2\n",
    "\n",
    "# Initialize encoder, decoder, and the model\n",
    "encoder = PhonemeEncoder(input_dim, embed_dim, hidden_dim, num_layers)\n",
    "decoder = WordDecoder(output_dim, embed_dim, hidden_dim, num_layers)\n",
    "Phoneme_Word_model = PhonemeToWordModel(encoder, decoder)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)  # 0 for padding\n",
    "optimizer = optim.Adam(Phoneme_Word_model.parameters(), lr=0.001)\n",
    "\n",
    "# Example batch\n",
    "batch_size = 16\n",
    "seq_len = 15  # Phoneme sequence length\n",
    "target_len = 10  # Word character sequence length\n",
    "\n",
    "src = torch.randint(1, input_dim, (batch_size, seq_len))  # Phoneme input (random example)\n",
    "trg = torch.randint(1, output_dim, (batch_size, target_len))  # Target word characters (random example)\n",
    "\n",
    "# Forward pass\n",
    "output = Phoneme_Word_model(src, trg)\n",
    "print(output.shape)\n",
    "# Calculate loss (reshape output for CrossEntropyLoss)\n",
    "output = output.view(-1, output_dim)\n",
    "trg = trg[:, 1:].reshape(-1)\n",
    "loss = criterion(output, trg)\n",
    "print(f\"Loss: {loss.item()}\")\n",
    "\n",
    "# Backward pass\n",
    "loss.backward()\n",
    "optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e0a37690-768e-4ee2-86c2-cb1f353ff366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of trg before reshape: torch.Size([160])\n",
      "Predicted words: ['orderfeelingsinquiredinquiredinquiredinquiredinquiredinquiredinquired', 'orderbarriersbarriersdesignedwoundedstandlipsstandlips', 'ordersearcheddesignedstandstandlipsstandlipsstand', 'sickinquiredinquiredinquiredinquiredinquiredinquiredrequiredinquired', 'ordersearchedgotstandstandlipsstandfrostherein', 'sicksickinquiredinquiredinquiredinquiredinquiredinquiredlips', 'greatlybooksellerstandstandfrosthereinstandstandfrost', 'chosechosechosechosechosechoseapologizerichdesigned', 'greatlygreatlyyouthfulstandstandlipsstandfrostherein', 'sickfixedfixeddesignedsearchedwoundedwoundedwoundedwounded', 'greatlyclosesearchedstandstandwoundedwoundedwoundeddesigned', 'sickinquiredinquiredinquiredinquiredinquiredlipslipsdivinity', 'feelingsfeelingsinquiredinquiredinquiredinquiredinquiredbelievedinquired', 'sicksickinquiredinquiredinquiredinquiredinquiredrequiredhazard', 'barriersbarriersbarriersinquiredinquiredinquiredlipsapologizeapologize', 'ordergotstandlipsstandlipsstandprovedlips']\n"
     ]
    }
   ],
   "source": [
    "# Example batch dimensions\n",
    "batch_size = 16\n",
    "target_len = 10  # Define your target length\n",
    "\n",
    "# Example target tensor (random values)\n",
    "trg = torch.randint(1, output_dim, (batch_size * target_len,))  # Ensure it's a 1D tensor for this example\n",
    "\n",
    "# Print the shape of trg to verify\n",
    "print(f\"Shape of trg before reshape: {trg.shape}\")\n",
    "\n",
    "# Reshape trg to (batch_size, target_length)\n",
    "trg = trg.view(batch_size, target_len)\n",
    "\n",
    "# Now call the model\n",
    "output = Phoneme_Word_model(src, trg)\n",
    "\n",
    "# Proceed with the rest of the code to convert output indices to characters\n",
    "\n",
    "\n",
    "# Assuming output shape is (batch_size, target_len, output_dim)\n",
    "# You can apply softmax to get probabilities and then use argmax to get indices\n",
    "output = output.view(-1, output_dim)  # Reshape for CrossEntropyLoss\n",
    "_, predicted_indices = torch.max(output, dim=1)  # Get the indices of the max probabilities\n",
    "\n",
    "# Reshape predicted_indices back to the shape of (batch_size, target_len - 1)\n",
    "predicted_indices = predicted_indices.view(batch_size, target_len - 1)\n",
    "\n",
    "# Convert predicted indices to characters\n",
    "predicted_words = []\n",
    "for i in range(predicted_indices.size(0)):\n",
    "    word = ''.join(index_to_word[idx.item()] for idx in predicted_indices[i] if idx.item() != 0)  # Ignore padding index\n",
    "    predicted_words.append(word)\n",
    "\n",
    "# Print the predicted words\n",
    "print(\"Predicted words:\", predicted_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fe85ea69-70de-4437-a8ef-621d92673467",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_sequences(mapping, phoneme_to_index, word_to_index):\n",
    "    phoneme_seq = [phoneme_to_index[phoneme] for phoneme, _ in mapping]\n",
    "    word_seq = [word_to_index[word] for _, word in mapping]  # Map each whole word to its index\n",
    "    return phoneme_seq, word_seq\n",
    "\n",
    "# Define phoneme to index mapping\n",
    "phoneme_to_index = {phoneme: idx for idx, phoneme in enumerate(all_phonemes)}\n",
    "word_to_index = {word: idx for idx, word in enumerate(all_words)}\n",
    "\n",
    "# Initialize training data\n",
    "training_data = []\n",
    "for path, mapping in phoneme_word_mappings.items():\n",
    "    phoneme_seq, word_seq = map_to_sequences(mapping, phoneme_to_index, word_to_index)\n",
    "    training_data.append((phoneme_seq, word_seq))  # Add to training data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c4cc1e4b-9b9b-4a4d-a859-d7f1935cea15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "38c43687-12f2-4b02-ad4d-24a5211f6f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0b5a6aac-25c0-40e4-a888-3bd7f08f6bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Assuming `phoneme_sequences` and `word_sequences` are lists of sequences (each sequence is a tensor of indices)\n",
    "# Pad phoneme and word sequences in the batch to the same length\n",
    "def pad_sequences(phoneme_sequences, word_sequences, pad_token=0):\n",
    "    padded_phoneme_sequences = pad_sequence(phoneme_sequences, batch_first=True, padding_value=pad_token)\n",
    "    padded_word_sequences = pad_sequence(word_sequences, batch_first=True, padding_value=pad_token)\n",
    "    return padded_phoneme_sequences, padded_word_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f1821d0b-0bf6-4640-9acc-efec229f01a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "\n",
    "class PhonemeWordDataset(Dataset):\n",
    "    def __init__(self, phoneme_sequences, word_sequences):\n",
    "        self.phoneme_sequences = phoneme_sequences\n",
    "        self.word_sequences = word_sequences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.phoneme_sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.phoneme_sequences[idx], self.word_sequences[idx]\n",
    "\n",
    "# Custom collate function to pad sequences in each batch\n",
    "def collate_fn(batch):\n",
    "    phoneme_seqs, word_seqs = zip(*batch)\n",
    "    padded_phoneme_seqs, padded_word_seqs = pad_sequences(phoneme_seqs, word_seqs, pad_token=0)\n",
    "    return padded_phoneme_seqs, padded_word_seqs\n",
    "\n",
    "# Extract phoneme and word sequences from training data\n",
    "phoneme_sequences = [torch.tensor(seq[0]) for seq in training_data]\n",
    "word_sequences = [torch.tensor(seq[1]) for seq in training_data]\n",
    "\n",
    "# Create the full dataset\n",
    "full_dataset = PhonemeWordDataset(phoneme_sequences, word_sequences)\n",
    "\n",
    "# Define the train-test split sizes\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size\n",
    "\n",
    "# Perform the random split\n",
    "train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n",
    "\n",
    "# Create DataLoader for training and test sets\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, collate_fn=collate_fn, shuffle=True, drop_last=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, collate_fn=collate_fn, shuffle=False, drop_last=True)\n",
    "\n",
    "# Now you can use train_dataloader and test_dataloader in your training and evaluation loops.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1daf9c-65ba-4475-a710-bd54fe362e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Set up device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Move model to the device\n",
    "model = Phoneme_Word_model.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding token\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Define the number of training epochs\n",
    "num_epochs = 10  # You can set this to any number based on how long you want to train\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Initialize accumulators for metrics\n",
    "    all_true_labels = []\n",
    "    all_pred_probs = []\n",
    "    \n",
    "    for src_batch, trg_batch in train_dataloader:\n",
    "        # Move to the same device as the model\n",
    "        src_batch, trg_batch = src_batch.to(device), trg_batch.to(device)\n",
    "\n",
    "        # Forward pass through the model\n",
    "        output = model(src_batch, trg_batch)\n",
    "\n",
    "        # Flatten the predictions for loss calculation\n",
    "        output = output.contiguous().view(-1, output.shape[-1])  # Shape: (batch_size * seq_len, output_dim)\n",
    "\n",
    "        # Flatten the target sequences after removing the start token\n",
    "        trg_batch = trg_batch[:, 1:].contiguous().view(-1)  # Shape: (batch_size * seq_len,)\n",
    "\n",
    "        # Check if target indices are within the range of output classes (ignoring padding tokens)\n",
    "        trg_batch[trg_batch >= output.shape[-1]] = output.shape[-1] - 1\n",
    "\n",
    "        # Calculate loss (ignore padding tokens)\n",
    "        loss = criterion(output, trg_batch)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Convert output logits to probabilities\n",
    "        pred_probs = torch.softmax(output, dim=1)\n",
    "\n",
    "        # Convert target labels and predicted probabilities to numpy arrays\n",
    "        true_labels = trg_batch.cpu().numpy()\n",
    "        pred_probs = pred_probs.cpu().detach().numpy()\n",
    "\n",
    "        # Append the current batch's true labels and predicted probabilities\n",
    "        all_true_labels.append(true_labels)\n",
    "        print(len(all_true_labels))\n",
    "        all_pred_probs.append(pred_probs)\n",
    "        print(len(all_true_labels))\n",
    "\n",
    "    # Flatten the lists of true labels and predicted probabilities for calculating metrics\n",
    "    all_true_labels = np.concatenate(all_true_labels)\n",
    "    all_pred_probs = np.concatenate(all_pred_probs)\n",
    "\n",
    "    # Calculate F1 score (ignore padding tokens)\n",
    "    # You can calculate the F1 score on non-padding classes by using `average='macro'`\n",
    "    f1 = f1_score(all_true_labels, np.argmax(all_pred_probs, axis=1), average='macro', zero_division=1)\n",
    "    \n",
    "    # Calculate Macro AUC\n",
    "    auc = roc_auc_score(all_true_labels, all_pred_probs, average='macro', multi_class='ovr')\n",
    "\n",
    "    print(f'Epoch {epoch + 1}, Loss: {loss.item()}, F1 Score: {f1:.4f}, Macro AUC: {auc:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dce228e9-4249-4685-b969-47516f91c4a8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Only one class present in y_true. ROC AUC score is not defined in that case.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 74\u001b[0m\n\u001b[0;32m     71\u001b[0m f1 \u001b[38;5;241m=\u001b[39m f1_score(all_true_labels, np\u001b[38;5;241m.\u001b[39margmax(all_pred_probs, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmacro\u001b[39m\u001b[38;5;124m'\u001b[39m, zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# Calculate Macro AUC\u001b[39;00m\n\u001b[1;32m---> 74\u001b[0m auc \u001b[38;5;241m=\u001b[39m \u001b[43mroc_auc_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_true_labels_one_hot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_pred_probs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmacro\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43movr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, F1 Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf1\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Macro AUC: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mauc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:648\u001b[0m, in \u001b[0;36mroc_auc_score\u001b[1;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[0;32m    640\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _average_binary_score(\n\u001b[0;32m    641\u001b[0m         partial(_binary_roc_auc_score, max_fpr\u001b[38;5;241m=\u001b[39mmax_fpr),\n\u001b[0;32m    642\u001b[0m         y_true,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    645\u001b[0m         sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[0;32m    646\u001b[0m     )\n\u001b[0;32m    647\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# multilabel-indicator\u001b[39;00m\n\u001b[1;32m--> 648\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_average_binary_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_binary_roc_auc_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_fpr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_fpr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    650\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    651\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    652\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    653\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    654\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_base.py:119\u001b[0m, in \u001b[0;36m_average_binary_score\u001b[1;34m(binary_metric, y_true, y_score, average, sample_weight)\u001b[0m\n\u001b[0;32m    117\u001b[0m     y_true_c \u001b[38;5;241m=\u001b[39m y_true\u001b[38;5;241m.\u001b[39mtake([c], axis\u001b[38;5;241m=\u001b[39mnot_average_axis)\u001b[38;5;241m.\u001b[39mravel()\n\u001b[0;32m    118\u001b[0m     y_score_c \u001b[38;5;241m=\u001b[39m y_score\u001b[38;5;241m.\u001b[39mtake([c], axis\u001b[38;5;241m=\u001b[39mnot_average_axis)\u001b[38;5;241m.\u001b[39mravel()\n\u001b[1;32m--> 119\u001b[0m     score[c] \u001b[38;5;241m=\u001b[39m \u001b[43mbinary_metric\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true_c\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_score_c\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscore_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;66;03m# Average the results\u001b[39;00m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:382\u001b[0m, in \u001b[0;36m_binary_roc_auc_score\u001b[1;34m(y_true, y_score, sample_weight, max_fpr)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Binary roc auc score.\"\"\"\u001b[39;00m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(np\u001b[38;5;241m.\u001b[39munique(y_true)) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m--> 382\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    383\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly one class present in y_true. ROC AUC score \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    384\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis not defined in that case.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    385\u001b[0m     )\n\u001b[0;32m    387\u001b[0m fpr, tpr, _ \u001b[38;5;241m=\u001b[39m roc_curve(y_true, y_score, sample_weight\u001b[38;5;241m=\u001b[39msample_weight)\n\u001b[0;32m    388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_fpr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m max_fpr \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[1;31mValueError\u001b[0m: Only one class present in y_true. ROC AUC score is not defined in that case."
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# Set up device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Move model to the device\n",
    "model = Phoneme_Word_model.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding token\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Define the number of training epochs\n",
    "num_epochs = 10  # You can set this to any number based on how long you want to train\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Initialize accumulators for metrics\n",
    "    all_true_labels = []\n",
    "    all_pred_probs = []\n",
    "    \n",
    "    for src_batch, trg_batch in train_dataloader:\n",
    "        # Move to the same device as the model\n",
    "        src_batch, trg_batch = src_batch.to(device), trg_batch.to(device)\n",
    "\n",
    "        # Forward pass through the model\n",
    "        output = model(src_batch, trg_batch)\n",
    "\n",
    "        # Flatten the predictions for loss calculation\n",
    "        output = output.contiguous().view(-1, output.shape[-1])  # Shape: (batch_size * seq_len, output_dim)\n",
    "\n",
    "        # Flatten the target sequences after removing the start token\n",
    "        trg_batch = trg_batch[:, 1:].contiguous().view(-1)  # Shape: (batch_size * seq_len,)\n",
    "\n",
    "        # Check if target indices are within the range of output classes (ignoring padding tokens)\n",
    "        trg_batch[trg_batch >= output.shape[-1]] = output.shape[-1] - 1\n",
    "\n",
    "        # Calculate loss (ignore padding tokens)\n",
    "        loss = criterion(output, trg_batch)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Convert output logits to probabilities\n",
    "        pred_probs = torch.softmax(output, dim=1)\n",
    "\n",
    "        # Convert target labels and predicted probabilities to numpy arrays\n",
    "        true_labels = trg_batch.cpu().numpy()\n",
    "        pred_probs = pred_probs.cpu().detach().numpy()\n",
    "\n",
    "        # Append the current batch's true labels and predicted probabilities\n",
    "        all_true_labels.append(true_labels)\n",
    "        all_pred_probs.append(pred_probs)\n",
    "\n",
    "    # Flatten the lists of true labels and predicted probabilities for calculating metrics\n",
    "    all_true_labels = np.concatenate(all_true_labels)\n",
    "    all_pred_probs = np.concatenate(all_pred_probs)\n",
    "\n",
    "    # Ensure all_true_labels is one-hot encoded (for multi-class AUC)\n",
    "    n_classes = all_pred_probs.shape[1]\n",
    "    all_true_labels_one_hot = np.zeros((all_true_labels.shape[0], n_classes))\n",
    "    all_true_labels_one_hot[np.arange(all_true_labels.shape[0]), all_true_labels] = 1\n",
    "\n",
    "    # Calculate F1 score (ignore padding tokens)\n",
    "    f1 = f1_score(all_true_labels, np.argmax(all_pred_probs, axis=1), average='macro', zero_division=1)\n",
    "\n",
    "    # Calculate Macro AUC\n",
    "    auc = roc_auc_score(all_true_labels_one_hot, all_pred_probs, average='macro', multi_class='ovr')\n",
    "\n",
    "    print(f'Epoch {epoch + 1}, Loss: {loss.item()}, F1 Score: {f1:.4f}, Macro AUC: {auc:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace17f72-1b67-4247-a8cc-2745a5359828",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Set up device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Move model to the device\n",
    "model = Phoneme_Word_model.to(device)\n",
    "\n",
    "# Set the model to evaluation mode (important for inference)\n",
    "model.eval()\n",
    "\n",
    "# Initialize accumulators for metrics\n",
    "all_true_labels = []\n",
    "all_pred_probs = []\n",
    "\n",
    "# No gradient computation during testing\n",
    "with torch.no_grad():\n",
    "    # Iterate through the test DataLoader\n",
    "    for src_batch, trg_batch in test_dataloader:\n",
    "        # Move to the same device as the model\n",
    "        src_batch, trg_batch = src_batch.to(device), trg_batch.to(device)\n",
    "\n",
    "        # Forward pass through the model\n",
    "        output = model(src_batch, trg_batch)\n",
    "\n",
    "        # Flatten the predictions for loss calculation\n",
    "        output = output.contiguous().view(-1, output.shape[-1])  # Shape: (batch_size * seq_len, output_dim)\n",
    "\n",
    "        # Flatten the target sequences after removing the start token\n",
    "        trg_batch = trg_batch[:, 1:].contiguous().view(-1)  # Shape: (batch_size * seq_len,)\n",
    "\n",
    "        # Convert output logits to probabilities\n",
    "        pred_probs = torch.softmax(output, dim=1)\n",
    "\n",
    "        # Convert target labels and predicted probabilities to numpy arrays\n",
    "        true_labels = trg_batch.cpu().numpy()\n",
    "        pred_probs = pred_probs.cpu().detach().numpy()\n",
    "\n",
    "        # Append the current batch's true labels and predicted probabilities\n",
    "        all_true_labels.append(true_labels)\n",
    "        all_pred_probs.append(pred_probs)\n",
    "\n",
    "# Flatten the lists of true labels and predicted probabilities for calculating metrics\n",
    "all_true_labels = np.concatenate(all_true_labels)\n",
    "all_pred_probs = np.concatenate(all_pred_probs)\n",
    "\n",
    "# Calculate F1 score (ignore padding tokens)\n",
    "f1 = f1_score(all_true_labels, np.argmax(all_pred_probs, axis=1), average='macro', zero_division=1)\n",
    "\n",
    "# Calculate Macro AUC\n",
    "auc = roc_auc_score(all_true_labels, all_pred_probs, avera\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a081d21c-6826-4680-93df-41b561a641bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 7.770174980163574\n"
     ]
    }
   ],
   "source": [
    "# 1. Forward pass through MFCC to Phoneme LSTM\n",
    "batch_size=10\n",
    "sequence_length=10\n",
    "input_dim=13\n",
    "x = torch.randn(batch_size, sequence_length, input_dim)  # Example MFCC input\n",
    "output_phoneme_logits = MFCC_model(x)  # Output shape: (batch_size, seq_len, output_dim)\n",
    "\n",
    "# 2. Extract predicted phonemes by taking argmax\n",
    "predicted_phonemes = output_phoneme_logits.argmax(dim=-1)  # Shape: (batch_size, seq_len)\n",
    "\n",
    "# Ensure you have a valid input for Phoneme Encoder\n",
    "# Since the PhonemeEncoder expects phonemes as indices, we can directly use predicted_phonemes\n",
    "# 3. Prepare input for the phoneme-to-word model\n",
    "# We need to ensure `predicted_phonemes` has the appropriate shape\n",
    "src = predicted_phonemes  # This will be the input to the encoder\n",
    "\n",
    "# Prepare a target tensor for the word decoder (trg)\n",
    "# Here we assume you have a valid target tensor, which you would typically obtain from your dataset.\n",
    "# For demonstration, we'll create a random target for characters.\n",
    "target_len = 10  # Set your desired target length for word decoding\n",
    "trg = torch.randint(1, output_dim, (batch_size, target_len))  # Random target word characters\n",
    "\n",
    "# 4. Forward pass through the Phoneme-to-Word model\n",
    "output = Phoneme_Word_model(src, trg)\n",
    "\n",
    "# 5. Calculate loss (reshape output for CrossEntropyLoss)\n",
    "output = output.view(-1, output_dim)  # Reshape for loss calculation\n",
    "trg = trg[:, 1:].reshape(-1)  # Reshape target tensor (ignoring the start token)\n",
    "loss = criterion(output, trg)\n",
    "print(f\"Loss: {loss.item()}\")\n",
    "\n",
    "# 6. Backward pass\n",
    "loss.backward()\n",
    "optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a48142c9-1755-4924-b8a9-227499d51c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader,random_split\n",
    "\n",
    "class MFCCToWordDataset(Dataset):\n",
    "    def __init__(self, inputs_tensor, targets_word_tensor):\n",
    "        \"\"\"\n",
    "        Custom dataset for loading MFCC data (inputs) and word labels (targets).\n",
    "        \n",
    "        Args:\n",
    "            inputs_tensor (torch.Tensor): Tensor containing MFCC features of shape (num_sequences, seq_len, num_mfcc_features).\n",
    "            targets_word_tensor (torch.Tensor): Tensor containing target word labels of shape (num_sequences, seq_len).\n",
    "        \"\"\"\n",
    "        self.inputs = inputs_tensor\n",
    "        self.targets = targets_word_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the number of samples (sequences)\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return the input (MFCC sequence) and target (word label sequence) for the given index\n",
    "        return self.inputs[idx], self.targets[idx]\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the dataset\n",
    "full_dataset = MFCCToWordDataset(inputs_tensor, targets_word_tensor)\n",
    "\n",
    "# Define split sizes (e.g., 80% for training and 20% for testing)\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n",
    "\n",
    "# Create DataLoaders for training and testing\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=10, shuffle=False)\n",
    "\n",
    "# Now you can use `train_dataloader` for training and `test_dataloader` for evaluation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fb5eb709-3e03-4295-abcc-f5706aa39ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Loss: 6.8202, F1 Score: 0.0042, AUC: None\n",
      "Epoch [2/2], Loss: 6.1110, F1 Score: 0.0124, AUC: None\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "\n",
    "num_epochs=2\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    all_targets = []\n",
    "    all_predictions = []\n",
    "    all_probabilities = []\n",
    "\n",
    "    for batch_idx, (inputs_batch, targets_batch) in enumerate(train_dataloader):\n",
    "        # 1. Forward pass through MFCC-to-Phoneme LSTM\n",
    "        outputs_phoneme = MFCC_model(inputs_batch)  # Output shape: (batch_size, seq_len, phoneme_output_dim)\n",
    "\n",
    "        # 2. Extract predicted phonemes by taking argmax\n",
    "        predicted_phonemes = outputs_phoneme.argmax(dim=-1)  # Shape: (batch_size, seq_len)\n",
    "\n",
    "        # 3. Clip `predicted_phonemes` to stay within bounds for embedding\n",
    "        predicted_phonemes = torch.clamp(predicted_phonemes, 0, output_dim - 1)\n",
    "        \n",
    "        # Prepare input for the phoneme-to-word model\n",
    "        src = predicted_phonemes  # Input to the word model\n",
    "        trg = torch.clamp(targets_batch, 0, output_dim - 1)  # Ensure trg is within valid bounds\n",
    "\n",
    "        # 4. Forward pass through the Phoneme-to-Word model\n",
    "        output_word = Phoneme_Word_model(src, trg)  # Shape: (batch_size, seq_len, output_dim)\n",
    "        \n",
    "        # 5. Calculate loss (reshape output for CrossEntropyLoss)\n",
    "        output_word = output_word.view(-1, output_dim)  # Reshape for loss calculation\n",
    "        trg = trg[:, 1:].reshape(-1)  # Reshape target tensor (ignoring the start token)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(output_word, trg)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # 6. Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 7. Get predicted classes and probabilities for metrics\n",
    "        predictions = output_word.argmax(dim=-1)  # Class predictions\n",
    "        probabilities = F.softmax(output_word, dim=-1)  # Probability scores\n",
    "\n",
    "        # Flatten and accumulate for F1 and AUC computation\n",
    "        all_targets.extend(trg.cpu().numpy())\n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "        all_probabilities.extend(probabilities.detach().cpu().numpy())\n",
    "\n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(all_targets, all_predictions, average=\"weighted\")\n",
    "    \n",
    "    # Calculate AUC (if binary or multilabel; modify as needed for multiclass)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_targets, all_probabilities, multi_class=\"ovr\", average=\"weighted\")\n",
    "    except ValueError:\n",
    "        auc = None  # AUC may not be available for some cases\n",
    "\n",
    "    # Print the average loss and metrics for the epoch\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(train_dataloader):.4f}, F1 Score: {f1:.4f}, AUC: {auc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1d8c94fe-9063-4aaf-a600-4f7fafe2d163",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 27\u001b[0m\n\u001b[0;32m     24\u001b[0m trg \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mclamp(targets_batch, \u001b[38;5;241m0\u001b[39m, output_dim \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Ensure trg is within valid bounds\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# 4. Forward pass through the Phoneme-to-Word model\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m output_word \u001b[38;5;241m=\u001b[39m \u001b[43mPhoneme_Word_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrg\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Shape: (batch_size, seq_len, output_dim)\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# 5. Calculate loss (reshape output for CrossEntropyLoss)\u001b[39;00m\n\u001b[0;32m     30\u001b[0m output_word \u001b[38;5;241m=\u001b[39m output_word\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, output_dim)  \u001b[38;5;66;03m# Reshape for loss calculation\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[24], line 67\u001b[0m, in \u001b[0;36mPhonemeToWordModel.forward\u001b[1;34m(self, src, trg)\u001b[0m\n\u001b[0;32m     64\u001b[0m decoder_input \u001b[38;5;241m=\u001b[39m trg[:, \u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# Start token for decoder\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, trg\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)):\n\u001b[1;32m---> 67\u001b[0m     output, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoder_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mappend(output\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     69\u001b[0m     decoder_input \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Use predicted character as input in the next step\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[24], line 44\u001b[0m, in \u001b[0;36mWordDecoder.forward\u001b[1;34m(self, x, hidden, encoder_outputs)\u001b[0m\n\u001b[0;32m     41\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Add a time dimension\u001b[39;00m\n\u001b[0;32m     42\u001b[0m embedded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(x)  \u001b[38;5;66;03m# Convert characters/words to embeddings\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Compute attention weights\u001b[39;00m\n\u001b[0;32m     45\u001b[0m attn_applied \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbmm(attn_weights\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m), encoder_outputs)  \u001b[38;5;66;03m# Apply weights to encoder outputs\u001b[39;00m\n\u001b[0;32m     47\u001b[0m rnn_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((embedded, attn_applied), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# Combine with embeddings for RNN input\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[24], line 27\u001b[0m, in \u001b[0;36mAttention.forward\u001b[1;34m(self, hidden, encoder_outputs)\u001b[0m\n\u001b[0;32m     25\u001b[0m seq_len \u001b[38;5;241m=\u001b[39m encoder_outputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     26\u001b[0m hidden \u001b[38;5;241m=\u001b[39m hidden\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m1\u001b[39m, seq_len, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# (batch_size, seq_len, hidden_dim)\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m energy \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtanh(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     28\u001b[0m attention \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv(energy)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# (batch_size, seq_len)\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39msoftmax(attention, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "\n",
    "#Just to test if this is working, remove the above line and replace when doing train-test split\n",
    "# Testing loop\n",
    "with torch.no_grad():\n",
    "    running_loss = 0.0\n",
    "    all_targets = []\n",
    "    all_predictions = []\n",
    "    all_probabilities = []\n",
    "\n",
    "    for batch_idx, (inputs_batch, targets_batch) in enumerate(test_dataloader):\n",
    "        # 1. Forward pass through MFCC-to-Phoneme LSTM\n",
    "        outputs_phoneme = MFCC_model(inputs_batch)  # Output shape: (batch_size, seq_len, phoneme_output_dim)\n",
    "\n",
    "        # 2. Extract predicted phonemes by taking argmax\n",
    "        predicted_phonemes = outputs_phoneme.argmax(dim=-1)  # Shape: (batch_size, seq_len)\n",
    "\n",
    "        # 3. Clip `predicted_phonemes` to stay within bounds for embedding\n",
    "        predicted_phonemes = torch.clamp(predicted_phonemes, 0, output_dim - 1)\n",
    "\n",
    "        # Prepare input for the phoneme-to-word model\n",
    "        src = predicted_phonemes  # Input to the word model\n",
    "        trg = torch.clamp(targets_batch, 0, output_dim - 1)  # Ensure trg is within valid bounds\n",
    "\n",
    "        # 4. Forward pass through the Phoneme-to-Word model\n",
    "        output_word = Phoneme_Word_model(src, trg)  # Shape: (batch_size, seq_len, output_dim)\n",
    "        \n",
    "        # 5. Calculate loss (reshape output for CrossEntropyLoss)\n",
    "        output_word = output_word.view(-1, output_dim)  # Reshape for loss calculation\n",
    "        trg = trg[:, 1:].reshape(-1)  # Reshape target tensor (ignoring the start token)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(output_word, trg)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # 6. Get predicted classes and probabilities for metrics\n",
    "        predictions = output_word.argmax(dim=-1)  # Class predictions\n",
    "        probabilities = F.softmax(output_word, dim=-1)  # Probability scores\n",
    "\n",
    "        # Flatten and accumulate for F1 and AUC computation\n",
    "        all_targets.extend(trg.cpu().numpy())\n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "        all_probabilities.extend(probabilities.cpu().numpy())\n",
    "\n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(all_targets, all_predictions, average=\"weighted\")\n",
    "    \n",
    "    # Calculate AUC (if binary or multilabel; modify as needed for multiclass)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_targets, all_probabilities, multi_class=\"ovr\", average=\"weighted\")\n",
    "    except ValueError:\n",
    "        auc = None  # AUC may not be available for some cases\n",
    "\n",
    "    # Print the average loss and metrics\n",
    "    print(f\"Test Loss: {running_loss / len(test_dataloader):.4f}, F1 Score: {f1:.4f}, AUC: {auc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0536eadd-ec70-42ab-bcb8-f284d4f3e435",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "\n",
    "def audio_to_word(audio_path, MFCC_model, Phoneme_Word_model, word_label_encoder, output_dim, mfcc_transform):\n",
    "    \"\"\"\n",
    "    Converts an audio file to a sequence of words using a combined MFCC-to-Phoneme model \n",
    "    and Phoneme-to-Word model pipeline.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    audio_path : str\n",
    "        Path to the audio file to be processed.\n",
    "    MFCC_model : torch.nn.Module\n",
    "        Neural network model that takes MFCC features as input and outputs phoneme predictions.\n",
    "    Phoneme_Word_model : torch.nn.Module\n",
    "        Neural network model that takes phoneme indices as input and outputs word character predictions.\n",
    "    word_label_encoder : sklearn.preprocessing.LabelEncoder\n",
    "        Encoder to map word character indices to actual characters/words.\n",
    "    output_dim : int\n",
    "        The output dimension of the Phoneme-to-Word model (number of unique characters/words).\n",
    "    mfcc_transform : torchaudio.transforms.MFCC\n",
    "        A transformation to convert audio waveform to MFCC features.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    predicted_words : list of str\n",
    "        The predicted sequence of words derived from the audio input.\n",
    "\n",
    "    Workflow:\n",
    "    ---------\n",
    "    1. Load audio from `audio_path` and convert to MFCC features using `mfcc_transform`.\n",
    "    2. Perform a forward pass through the `MFCC_model` to predict phonemes.\n",
    "    3. Clip predicted phonemes to be within valid bounds for the Phoneme-to-Word model.\n",
    "    4. Pass the predicted phonemes to the `Phoneme_Word_model` along with a dummy target for autoregressive decoding.\n",
    "    5. Map the output indices to words using `word_label_encoder`.\n",
    "\n",
    "    Example:\n",
    "    --------\n",
    "    >>> mfcc_transform = torchaudio.transforms.MFCC(sample_rate=16000, n_mfcc=13)\n",
    "    >>> predicted_words = audio_to_word(\"audio.wav\", MFCC_model, Phoneme_Word_model, word_label_encoder, output_dim, mfcc_transform)\n",
    "    >>> print(predicted_words)\n",
    "    ['hello', 'world']\n",
    "    \"\"\"\n",
    "    # Step 1: Load audio and convert to MFCC\n",
    "    waveform, sample_rate = torchaudio.load(audio_path)\n",
    "    mfcc_features = mfcc_transform(waveform).transpose(1, 2)  # Shape: (batch_size=1, seq_len, mfcc_dim)\n",
    "\n",
    "    # Step 2: Forward pass through MFCC-to-Phoneme model\n",
    "    output_phoneme_logits = MFCC_model(mfcc_features)  # Shape: (1, seq_len, phoneme_output_dim)\n",
    "    predicted_phonemes = output_phoneme_logits.argmax(dim=-1)  # Shape: (1, seq_len)\n",
    "\n",
    "    # Step 3: Prepare phonemes for Phoneme-to-Word model\n",
    "    src = predicted_phonemes\n",
    "\n",
    "    # Step 4: Generate target tensor (dummy input for decoding without labels)\n",
    "    target_len = src.size(1)  # Set desired target length based on the sequence length of `src`\n",
    "    trg = torch.zeros(1, target_len, dtype=torch.long)  # Shape: (1, target_len), initialized with zeros\n",
    "\n",
    "    # Step 5: Forward pass through Phoneme-to-Word model to obtain words\n",
    "    output_word_logits = Phoneme_Word_model(src, trg)  # Shape: (1, target_len, output_dim)\n",
    "    predicted_word_indices = output_word_logits.argmax(dim=-1).squeeze(0)  # Shape: (target_len,)\n",
    "\n",
    "    # Step 6: Convert predicted indices to words\n",
    "    predicted_words = [word_label_encoder.inverse_transform([index])[0] for index in predicted_word_indices]\n",
    "\n",
    "    return predicted_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4d28b8d5-d279-49b6-927f-5d2b839093ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchaudio\\functional\\functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[np.str_('had'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " np.str_('and'),\n",
       " ...]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Defining mfcc_transform\n",
    "import torchaudio\n",
    "\n",
    "# Define the MFCC transformation with appropriate settings\n",
    "mfcc_transform = torchaudio.transforms.MFCC(\n",
    "    sample_rate=16000,   # Replace with your actual sample rate if different\n",
    "    n_mfcc=13            # Number of MFCC coefficients\n",
    ")\n",
    "\n",
    "audio_to_word(\"C:/Users/siddh/Downloads/Voice-001.wav\", MFCC_model, Phoneme_Word_model, word_label_encoder, output_dim, mfcc_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0458c3c-059a-4812-966c-c31703bd9b66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
