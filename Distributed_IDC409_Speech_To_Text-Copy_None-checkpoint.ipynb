{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59d2e807-0e10-4489-b3f7-255e23483fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Montreal Forced Alignment to create test and training data:\n",
    "\n",
    "#Using conda prompt\n",
    "\n",
    "#conda create -n conda_env_name (Creating an anaconda environment)\n",
    "\n",
    "#conda activate conda_env_name (Activating anaconda environment)\n",
    "\n",
    "#conda install -c conda-forge montreal-forced-aligner (Installing Montreal Forced Aligner in our environment)\n",
    "\n",
    "#mfa model download acoustic english_us_arpa (Downloading acoustic model)\n",
    "\n",
    "#mfa model download  dictionary english_us_arpa (Downloading dictionary)\n",
    "\n",
    "#mfa align path/to/Formatted_Corpus english_us_arpa english_us_arpa output path (Generates times that phonemes and words are present as a textgrid format file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1bf1a27c-27c1-462f-a798-301af54489ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available tiers: ['words', 'phones']\n",
      "Tier 'phonemes' not found in the TextGrid file.\n"
     ]
    }
   ],
   "source": [
    "from textgrid import TextGrid\n",
    "\n",
    "# Load the TextGrid file\n",
    "textgrid_path = \"C:/Users/siddh/Documents/aligned_librispeech/19/19-198-0000.TextGrid\"\n",
    "tg = TextGrid.fromFile(textgrid_path)\n",
    "\n",
    "# Print available tier names to confirm\n",
    "print(\"Available tiers:\", [tier.name for tier in tg.tiers])\n",
    "\n",
    "# Access the correct tier name (e.g., \"phonemes\" or \"words\")\n",
    "tier_name = \"phonemes\"  # Replace with the actual name of your tier\n",
    "tier = tg.getFirst(tier_name)\n",
    "\n",
    "# Check if the tier exists\n",
    "if tier is not None:\n",
    "    # Extract intervals (start time, end time, label) from the tier\n",
    "    alignment_data = []\n",
    "    for interval in tier.intervals:\n",
    "        alignment_data.append({\n",
    "            \"label\": interval.mark,\n",
    "            \"start_time\": interval.minTime,\n",
    "            \"end_time\": interval.maxTime\n",
    "        })\n",
    "\n",
    "    # Display the extracted alignment data\n",
    "    for entry in alignment_data:\n",
    "        print(f\"Label: {entry['label']}, Start: {entry['start_time']}, End: {entry['end_time']}\")\n",
    "else:\n",
    "    print(f\"Tier '{tier_name}' not found in the TextGrid file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "281b6e1b-dcaa-4748-84fb-3b4b377545b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextGrid(None, [IntervalTier(words, [Interval(0.0, 0.57, None), Interval(0.57, 1.25, northanger), Interval(1.25, 1.3, None), Interval(1.3, 1.77, abbey), Interval(1.77, 1.965, None)]), IntervalTier(phones, [Interval(0.0, 0.57, None), Interval(0.57, 0.69, N), Interval(0.69, 0.73, AO2), Interval(0.73, 0.82, R), Interval(0.82, 0.91, TH), Interval(0.91, 1.05, AE1), Interval(1.05, 1.13, NG), Interval(1.13, 1.25, ER0), Interval(1.25, 1.3, None), Interval(1.3, 1.46, AE1), Interval(1.46, 1.54, B), Interval(1.54, 1.77, IY0), Interval(1.77, 1.965, None)])])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textgrid import TextGrid\n",
    "\n",
    "# Load the TextGrid file\n",
    "textgrid_path = \"C:/Users/siddh/Documents/aligned_librispeech/19/19-198-0000.TextGrid\"\n",
    "tg = TextGrid.fromFile(textgrid_path)\n",
    "tg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5ff83370-2f19-40c2-a20e-97e704c694d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': None,\n",
       " 'minTime': 0.0,\n",
       " 'maxTime': 1.965,\n",
       " 'tiers': [IntervalTier(words, [Interval(0.0, 0.57, None), Interval(0.57, 1.25, northanger), Interval(1.25, 1.3, None), Interval(1.3, 1.77, abbey), Interval(1.77, 1.965, None)]),\n",
       "  IntervalTier(phones, [Interval(0.0, 0.57, None), Interval(0.57, 0.69, N), Interval(0.69, 0.73, AO2), Interval(0.73, 0.82, R), Interval(0.82, 0.91, TH), Interval(0.91, 1.05, AE1), Interval(1.05, 1.13, NG), Interval(1.13, 1.25, ER0), Interval(1.25, 1.3, None), Interval(1.3, 1.46, AE1), Interval(1.46, 1.54, B), Interval(1.54, 1.77, IY0), Interval(1.77, 1.965, None)])],\n",
       " 'strict': True}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tg.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2a39fbf3-1470-4287-8b2e-63251a0aba45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IntervalTier(words, [Interval(0.0, 0.57, None), Interval(0.57, 1.25, northanger), Interval(1.25, 1.3, None), Interval(1.3, 1.77, abbey), Interval(1.77, 1.965, None)])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tg.tiers\n",
    "tg.tiers[0]\n",
    "tg.tiers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "13c382f3-778a-4cbc-ae5f-c0a2879d03cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from textgrid import TextGrid\n",
    "\n",
    "# Function to parse TextGrid files and store data in a dictionary\n",
    "def parse_textgrid_to_dict(textgrid_path):\n",
    "    # Initialize dictionary to hold alignment data for the file\n",
    "    alignment_data = {}\n",
    "\n",
    "    # Load the TextGrid file\n",
    "    tg = TextGrid.fromFile(textgrid_path)\n",
    "    \n",
    "    # Define the tiers you want to parse\n",
    "    tier_names = [\"phones\", \"words\"]  # Replace with your TextGrid's actual tier names\n",
    "\n",
    "    for tier_name in tier_names:\n",
    "        # Try to get the specified tier, skip if it doesn't exist\n",
    "        tier = tg.getFirst(tier_name)\n",
    "        if tier is None:\n",
    "            #print(f\"Tier '{tier_name}' not found in {textgrid_path}. Skipping this tier.\")\n",
    "            continue\n",
    "\n",
    "        # Initialize a list to store intervals for this tier\n",
    "        tier_data = []\n",
    "        for interval in tier.intervals:\n",
    "            # Append interval data to the list, using \"N/A\" for empty labels\n",
    "            tier_data.append({\n",
    "                \"label\": interval.mark if interval.mark else \"N/A\",\n",
    "                \"start_time\": interval.minTime,\n",
    "                \"end_time\": interval.maxTime\n",
    "            })\n",
    "        \n",
    "        # Add tier data to alignment data\n",
    "        alignment_data[tier_name] = tier_data\n",
    "    \n",
    "    return alignment_data\n",
    "\n",
    "# Main function to parse all TextGrids in the directory structure and store in a dictionary\n",
    "def parse_all_textgrids_to_dict(base_directory):\n",
    "    all_alignments = {}\n",
    "    \n",
    "    # Walk through all folders and files\n",
    "    for root, dirs, files in os.walk(base_directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".TextGrid\"):\n",
    "                textgrid_path = os.path.join(root, file)\n",
    "                #print(f\"Parsing {textgrid_path}\")\n",
    "\n",
    "                # Parse the TextGrid file and add it to the dictionary\n",
    "                alignment_data = parse_textgrid_to_dict(textgrid_path)\n",
    "                all_alignments[textgrid_path] = alignment_data\n",
    "\n",
    "    return all_alignments\n",
    "\n",
    "# Specify the base directory containing your folders and TextGrid files\n",
    "base_directory = \"C:/Users/siddh/Documents/aligned_librispeech\"\n",
    "all_alignments = parse_all_textgrids_to_dict(base_directory)\n",
    "\n",
    "# Display parsed data for verification\n",
    "for textgrid_path, alignment_data in all_alignments.items():\n",
    "    #print(f\"\\nParsed data for {textgrid_path}:\")\n",
    "    pass\n",
    "    for tier, intervals in alignment_data.items():\n",
    "        #print(f\"  Tier: {tier}\")\n",
    "        pass\n",
    "        for interval in intervals:\n",
    "            pass\n",
    "            #print(f\"    Label: {interval['label']}, Start: {interval['start_time']}, End: {interval['end_time']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "69cc71ef-5e2c-4a58-82b9-3e57dfb8de3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_alignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a7585c0e-e4f0-41d7-beb0-12eb7d52f641",
   "metadata": {},
   "outputs": [],
   "source": [
    "#formatted_alignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b2d52f13-0db5-4f18-b0c0-f16c5f1262b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def convert_alignments_from_textgrids(all_alignments, base_audio_dir=\"C:/Users/siddh/Downloads/Librispeech/Librispeech\"):\n",
    "    # Dictionary to store processed alignments and corresponding audio path\n",
    "    formatted_alignments = {}\n",
    "\n",
    "    for textgrid_path, tiers_data in all_alignments.items():\n",
    "        # Extract \"phones\" tier data if available\n",
    "        phoneme_intervals = [\n",
    "            (interval[\"start_time\"], interval[\"end_time\"], interval[\"label\"])\n",
    "            for interval in tiers_data.get(\"phones\", [])\n",
    "            if interval[\"label\"] != \"N/A\"  # Ignore unmarked intervals\n",
    "        ]\n",
    "        \n",
    "        # Extract \"words\" tier data if available\n",
    "        word_intervals = [\n",
    "            (interval[\"start_time\"], interval[\"end_time\"], interval[\"label\"])\n",
    "            for interval in tiers_data.get(\"words\", [])\n",
    "            if interval[\"label\"] != \"N/A\"  # Ignore unmarked intervals\n",
    "        ]\n",
    "        \n",
    "        # Derive the audio path based on TextGrid path identifier\n",
    "        identifier = os.path.basename(textgrid_path).replace(\".TextGrid\", \"\")\n",
    "        audio_file_path = os.path.join(base_audio_dir, identifier[:2], f\"{identifier}.wav\")\n",
    "\n",
    "        # Check if the corresponding audio file exists\n",
    "        if not os.path.exists(audio_file_path):\n",
    "            print(f\"Audio file not found for {identifier}: {audio_file_path}\")\n",
    "            continue\n",
    "        \n",
    "        # Store phoneme intervals, word intervals, and audio path in the dictionary\n",
    "        formatted_alignments[textgrid_path] = {\n",
    "            \"audio_path\": audio_file_path,\n",
    "            \"phoneme_intervals\": phoneme_intervals,\n",
    "            \"word_intervals\": word_intervals\n",
    "        }\n",
    "    \n",
    "    return formatted_alignments\n",
    "\n",
    "# Example usage\n",
    "formatted_alignments = convert_alignments_from_textgrids(all_alignments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "797943fa-2325-4aa3-abcc-4c0234b60673",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'audio_path': 'C:/Users/siddh/Downloads/Librispeech/Librispeech\\\\19\\\\19-198-0000.wav',\n",
       " 'phoneme_intervals': [(0.57, 0.69, 'N'),\n",
       "  (0.69, 0.73, 'AO2'),\n",
       "  (0.73, 0.82, 'R'),\n",
       "  (0.82, 0.91, 'TH'),\n",
       "  (0.91, 1.05, 'AE1'),\n",
       "  (1.05, 1.13, 'NG'),\n",
       "  (1.13, 1.25, 'ER0'),\n",
       "  (1.3, 1.46, 'AE1'),\n",
       "  (1.46, 1.54, 'B'),\n",
       "  (1.54, 1.77, 'IY0')],\n",
       " 'word_intervals': [(0.57, 1.25, 'northanger'), (1.3, 1.77, 'abbey')]}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(formatted_alignments.values())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e2fc4fb8-b0f1-4d57-a9d2-63b15160916a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "def get_mfcc_with_labels(formatted_alignments, sr=16000, mfcc_dim=13):\n",
    "    phoneme_data_with_labels = []\n",
    "    word_data_with_labels = []\n",
    "\n",
    "    for entry in formatted_alignments.values():\n",
    "        # Check for audio path\n",
    "        if \"audio_path\" not in entry:\n",
    "            raise KeyError(\"Each entry in formatted_alignments must contain 'audio_path' to locate audio files.\")\n",
    "        \n",
    "        audio_path = entry[\"audio_path\"]\n",
    "        phoneme_intervals = entry[\"phoneme_intervals\"]\n",
    "        word_intervals = entry[\"word_intervals\"]  # Assuming word-level intervals are included\n",
    "\n",
    "        # Load audio file\n",
    "        audio, _ = librosa.load(audio_path, sr=sr)\n",
    "\n",
    "        # Extract MFCC features (shape: (num_frames, mfcc_dim))\n",
    "        mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=mfcc_dim).T  # Transpose to shape (num_frames, mfcc_dim)\n",
    "\n",
    "        # Calculate frame duration in seconds\n",
    "        frame_duration = len(audio) / len(mfcc) / sr  # Duration of each MFCC frame\n",
    "\n",
    "        # Initialize lists for phoneme and word labels\n",
    "        mfcc_phoneme_labels = []\n",
    "        mfcc_word_labels = []\n",
    "        \n",
    "        for i, mfcc_frame in enumerate(mfcc):\n",
    "            frame_start_time = i * frame_duration\n",
    "\n",
    "            # Find the phoneme label based on frame start time\n",
    "            phoneme_label = next((label for start, end, label in phoneme_intervals if start <= frame_start_time < end), None)\n",
    "            if phoneme_label is not None:\n",
    "                mfcc_phoneme_labels.append((mfcc_frame, phoneme_label))\n",
    "\n",
    "            # Find the word label based on frame start time\n",
    "            word_label = next((label for start, end, label in word_intervals if start <= frame_start_time < end), None)\n",
    "            if word_label is not None:\n",
    "                mfcc_word_labels.append((mfcc_frame, word_label))\n",
    "\n",
    "        # Append data to respective lists\n",
    "        phoneme_data_with_labels.append({\n",
    "            \"audio_path\": audio_path,\n",
    "            \"mfcc_with_labels\": mfcc_phoneme_labels\n",
    "        })\n",
    "        \n",
    "        word_data_with_labels.append({\n",
    "            \"audio_path\": audio_path,\n",
    "            \"mfcc_with_labels\": mfcc_word_labels\n",
    "        })\n",
    "\n",
    "    return phoneme_data_with_labels, word_data_with_labels\n",
    "\n",
    "# Example usage\n",
    "mfcc_phoneme_data, mfcc_word_data = get_mfcc_with_labels(formatted_alignments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "967215d1-c693-4cc4-b17f-386a2f6a1cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mfcc_phoneme_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "78536383-c0cd-4773-9fb8-e8f0fb8be056",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mfcc_word_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4bcdf115-4693-4b13-8f5f-d25c2cdea3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if this is even doing anything\n",
    "def extract_unique_phonemes_and_words(formatted_alignments):\n",
    "    unique_phonemes = set()\n",
    "    unique_words = set()\n",
    "\n",
    "    # Iterate over each alignment entry\n",
    "    for alignment in formatted_alignments.values():\n",
    "        # Extract phoneme labels and add them to the phoneme set\n",
    "        for _, _, phoneme in alignment.get(\"phoneme_intervals\", []):\n",
    "            unique_phonemes.add(phoneme)\n",
    "\n",
    "        # Extract word labels and add them to the word set\n",
    "        for _, _, word in alignment.get(\"word_intervals\", []):\n",
    "            unique_words.add(word)\n",
    "\n",
    "    # Return both unique phonemes and unique words as sorted lists\n",
    "    return sorted(unique_phonemes), sorted(unique_words)\n",
    "\n",
    "# Usage\n",
    "unique_phonemes, unique_words = extract_unique_phonemes_and_words(formatted_alignments)\n",
    "#print(f\"Unique phonemes: {unique_phonemes}\")\n",
    "#print(f\"Unique words: {unique_words}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a3089b18-61dc-434d-ab8f-49eae6a92890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68\n"
     ]
    }
   ],
   "source": [
    "print(len(unique_phonemes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5104bec4-8e11-4fe4-b2d6-edecb9075637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2356\n"
     ]
    }
   ],
   "source": [
    "print(len(unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8e4ad44a-b189-4ea3-9b35-719aaaf5e1b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs shape: torch.Size([69165, 100, 13])\n",
      "Targets (phoneme) shape: torch.Size([69165, 100])\n",
      "Targets (word) shape: torch.Size([69165, 100])\n",
      "Unique phoneme classes: ['AA0' 'AA1' 'AA2' 'AE0' 'AE1' 'AE2' 'AH0' 'AH1' 'AH2' 'AO0' 'AO1' 'AO2'\n",
      " 'AW1' 'AW2' 'AY0' 'AY1' 'AY2' 'B' 'CH' 'D' 'DH' 'EH0' 'EH1' 'EH2' 'ER0'\n",
      " 'ER1' 'ER2' 'EY0' 'EY1' 'EY2' 'F' 'G' 'HH' 'IH0' 'IH1' 'IH2' 'IY0' 'IY1'\n",
      " 'IY2' 'JH' 'K' 'L' 'M' 'N' 'NG' 'OW0' 'OW1' 'OW2' 'OY0' 'OY1' 'P' 'R' 'S'\n",
      " 'SH' 'T' 'TH' 'UH0' 'UH1' 'UH2' 'UW0' 'UW1' 'UW2' 'V' 'W' 'Y' 'Z' 'ZH'\n",
      " 'spn']\n",
      "Unique word classes: ['a' 'abated' 'abatement' ... 'zee' 'zero' 'zuyder']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def prepare_data(mfcc_phoneme_data, mfcc_word_data, sequence_length):\n",
    "    \"\"\"\n",
    "    Prepares data for training by extracting MFCC, phoneme, and word labels,\n",
    "    encoding labels, and creating sequences of specified length.\n",
    "\n",
    "    Args:\n",
    "        mfcc_phoneme_data (list): List of dictionaries with MFCC and phoneme labels.\n",
    "        mfcc_word_data (list): List of dictionaries with MFCC and word labels.\n",
    "        sequence_length (int): Length of the sequences to create.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array of MFCC input sequences.\n",
    "        np.ndarray: Array of encoded phoneme target sequences.\n",
    "        np.ndarray: Array of encoded word target sequences.\n",
    "        LabelEncoder: Encoder for phoneme labels.\n",
    "        LabelEncoder: Encoder for word labels.\n",
    "    \"\"\"\n",
    "    # Lists to hold input-output pairs\n",
    "    inputs = []\n",
    "    targets_phoneme = []\n",
    "    targets_word = []\n",
    "\n",
    "    # Create lists to gather all phoneme and word labels for fitting the encoder\n",
    "    all_phoneme_labels = []\n",
    "    all_word_labels = []\n",
    "\n",
    "    # Extract phoneme labels from mfcc_phoneme_data\n",
    "    for entry in mfcc_phoneme_data:\n",
    "        mfcc_with_labels = entry[\"mfcc_with_labels\"]\n",
    "        phoneme_labels = [label for _, label in mfcc_with_labels]\n",
    "        all_phoneme_labels.extend(phoneme_labels)\n",
    "\n",
    "    # Extract word labels from mfcc_word_data\n",
    "    for entry in mfcc_word_data:\n",
    "        word_intervals = entry[\"mfcc_with_labels\"]\n",
    "        word_labels = [word for _, word in word_intervals]\n",
    "        all_word_labels.extend(word_labels)\n",
    "\n",
    "    # Fit label encoders on both phoneme and word labels\n",
    "    phoneme_label_encoder = LabelEncoder()\n",
    "    word_label_encoder = LabelEncoder()\n",
    "    phoneme_label_encoder.fit(all_phoneme_labels)\n",
    "    word_label_encoder.fit(all_word_labels)\n",
    "\n",
    "    # Iterate through the phoneme data to create sequences\n",
    "    for entry in mfcc_phoneme_data:\n",
    "        mfcc_with_labels = entry[\"mfcc_with_labels\"]\n",
    "        mfcc_array = np.array([mfcc for mfcc, _ in mfcc_with_labels])  # Convert MFCC features to array\n",
    "        phoneme_labels = [label for _, label in mfcc_with_labels]\n",
    "        encoded_phoneme_labels = phoneme_label_encoder.transform(phoneme_labels)\n",
    "\n",
    "        for start in range(len(mfcc_array) - sequence_length + 1):\n",
    "            end = start + sequence_length\n",
    "            inputs.append(mfcc_array[start:end])\n",
    "            targets_phoneme.append(encoded_phoneme_labels[start:end])\n",
    "\n",
    "    # Iterate through the word data to create sequences\n",
    "    for entry in mfcc_word_data:\n",
    "        word_intervals = entry[\"mfcc_with_labels\"]\n",
    "        word_labels = [word for _, word in word_intervals]\n",
    "        encoded_word_labels = word_label_encoder.transform(word_labels)\n",
    "\n",
    "        for start in range(len(encoded_word_labels) - sequence_length + 1):\n",
    "            end = start + sequence_length\n",
    "            targets_word.append(encoded_word_labels[start:end])\n",
    "\n",
    "    # Return inputs, targets (phoneme and word), and label encoders\n",
    "    return np.array(inputs), np.array(targets_phoneme), np.array(targets_word), phoneme_label_encoder, word_label_encoder\n",
    "\n",
    "# Example usage\n",
    "sequence_length = 100  # Define your desired sequence length\n",
    "inputs, targets_phoneme, targets_word, phoneme_label_encoder, word_label_encoder = prepare_data(\n",
    "    mfcc_phoneme_data, mfcc_word_data, sequence_length\n",
    ")\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "inputs_tensor = torch.tensor(inputs, dtype=torch.float32)  # Shape: (num_sequences, sequence_length, num_mfcc_features)\n",
    "targets_phoneme_tensor = torch.tensor(targets_phoneme, dtype=torch.long)  # Shape: (num_sequences, sequence_length)\n",
    "targets_word_tensor = torch.tensor(targets_word, dtype=torch.long)  # Shape: (num_sequences, sequence_length)\n",
    "\n",
    "# Check the shapes\n",
    "print(f\"Inputs shape: {inputs_tensor.shape}\")\n",
    "print(f\"Targets (phoneme) shape: {targets_phoneme_tensor.shape}\")\n",
    "print(f\"Targets (word) shape: {targets_word_tensor.shape}\")\n",
    "print(f\"Unique phoneme classes: {phoneme_label_encoder.classes_}\")\n",
    "print(f\"Unique word classes: {word_label_encoder.classes_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "83fb97f5-7f73-470c-b6de-35f05ebcaa34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 4.222255229949951\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class MFCCtoPhonemeLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=1):\n",
    "        super(MFCCtoPhonemeLSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Define the LSTM layer\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "\n",
    "        # Define the output layer that maps hidden state to phoneme classes\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden and cell state\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
    "\n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "\n",
    "        # Reshape out for the fully connected layer\n",
    "        out = out.reshape(-1, self.hidden_dim)\n",
    "\n",
    "        # Pass through the fully connected layer and return\n",
    "        out = self.fc(out)\n",
    "        out = out.view(x.size(0), -1, out.size(-1))  # reshape to (batch_size, seq_len, output_dim)\n",
    "        return out\n",
    "\n",
    "# Parameters\n",
    "input_dim = 13  # Number of MFCC features per frame\n",
    "hidden_dim = 128  # Size of the hidden layer in LSTM\n",
    "output_dim = phoneme_label_encoder.classes_.size  # Number of phoneme classes (or 61 if using the full set)\n",
    "num_layers = 2  # Number of LSTM layers\n",
    "\n",
    "# Instantiate the model, loss function, and optimizer\n",
    "MFCC_model = MFCCtoPhonemeLSTM(input_dim, hidden_dim, output_dim, num_layers)\n",
    "criterion = nn.CrossEntropyLoss()  # Loss function\n",
    "optimizer = optim.Adam(MFCC_model.parameters(), lr=0.001)\n",
    "\n",
    "# Example dummy input (batch_size, sequence_length, input_dim)\n",
    "batch_size = 16\n",
    "sequence_length = 100  # Number of frames in each sequence\n",
    "x = torch.randn(batch_size, sequence_length, input_dim)  # Random batch of MFCC features\n",
    "y = torch.randint(0, output_dim, (batch_size, sequence_length))  # Random phoneme labels\n",
    "\n",
    "# Forward pass\n",
    "output = MFCC_model(x)  # Output shape: (batch_size, seq_len, output_dim)\n",
    "\n",
    "# Calculate loss (we flatten the output and target to (batch_size * seq_len, output_dim) for cross-entropy)\n",
    "loss = criterion(output.view(-1, output_dim), y.view(-1))\n",
    "print(f\"Loss: {loss.item()}\")\n",
    "\n",
    "# Backward pass and optimize\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e2192861-16db-4195-a00a-bc39090c9436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted phonemes: ['T' 'T' 'T' 'AY2' 'AY2' 'AY2' 'AY2' 'AY2' 'AY2' 'W' 'W' 'W' 'AY2' 'AY2'\n",
      " 'W' 'AY2' 'W' 'W' 'W' 'AY2' 'AY2' 'AY2' 'AY2' 'AY2' 'W' 'AY2' 'AY2' 'W'\n",
      " 'W' 'AY2' 'AY2' 'AY2' 'AY2' 'AY2' 'AY2' 'AY2' 'W' 'AY2' 'W' 'W' 'W' 'T'\n",
      " 'EH0' 'Z' 'Z' 'Z' 'Z' 'UW2' 'UW2' 'W' 'W' 'W' 'W' 'W' 'W' 'W' 'W' 'W' 'W'\n",
      " 'W' 'W' 'W' 'W' 'UW0' 'UW0' 'UW0' 'UW0' 'UW0' 'UW0' 'W' 'W' 'W' 'AY2'\n",
      " 'AY2' 'AY2' 'W' 'W' 'W' 'W' 'UW0' 'EY0' 'W' 'AY2' 'AY2' 'AY2' 'W' 'W' 'W'\n",
      " 'W' 'AY2' 'AY2' 'EY0' 'EY0' 'EY0' 'EY0' 'UW0' 'EY0' 'W' 'EY0' 'EY0']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Assuming model and label_encoder are already defined\n",
    "\n",
    "def convert_indices_to_phonemes(predicted_indices, label_encoder):\n",
    "    \"\"\"\n",
    "    Converts model output (indices) into phoneme labels.\n",
    "    \n",
    "    predicted_indices: Tensor of shape (batch_size, seq_len) containing predicted phoneme indices\n",
    "    label_encoder: The LabelEncoder that maps phoneme indices to phoneme labels\n",
    "    \"\"\"\n",
    "    # Convert indices to phoneme labels\n",
    "    phoneme_labels = phoneme_label_encoder.inverse_transform(predicted_indices.cpu().numpy().flatten())\n",
    "    \n",
    "    # Reshape back to match the sequence structure (batch_size, seq_len)\n",
    "    phoneme_labels = phoneme_labels.reshape(predicted_indices.shape)\n",
    "    \n",
    "    return phoneme_labels\n",
    "\n",
    "# Example forward pass through the model\n",
    "# Assuming `inputs_tensor` contains your MFCC input sequences\n",
    "# (batch_size, seq_len, num_mfcc_features)\n",
    "model = MFCCtoPhonemeLSTM(input_dim=13, hidden_dim=64, output_dim=len(phoneme_label_encoder.classes_), num_layers=1)\n",
    "#inputs_tensor = inputs_tensor[:batch_size]  # Adjust this to match your data\n",
    "#Decoding_Entire Output at once takes a lot of memory\n",
    "Input_tensor_portion = inputs_tensor[:batch_size]\n",
    "predicted_indices = model(Input_tensor_portion)  # Output shape: (batch_size, seq_len, num_phonemes)\n",
    "\n",
    "# Apply softmax to get probabilities (optional, if you want confidence scores)\n",
    "softmax = torch.nn.Softmax(dim=-1)\n",
    "probabilities = softmax(predicted_indices)\n",
    "\n",
    "# Get the predicted phoneme indices (max probability per frame)\n",
    "predicted_phoneme_indices = torch.argmax(probabilities, dim=-1)  # Shape: (batch_size, seq_len)\n",
    "\n",
    "# Convert the predicted indices to phoneme labels\n",
    "predicted_phonemes = convert_indices_to_phonemes(predicted_phoneme_indices, phoneme_label_encoder)\n",
    "\n",
    "# Print the phonemes for the first example (for debugging purposes)\n",
    "print(f\"Predicted phonemes: {predicted_phonemes[0]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0922256d-426a-406d-8e92-735b9a18eb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split, DataLoader\n",
    "import torch\n",
    "\n",
    "# Define the full dataset\n",
    "full_dataset = torch.utils.data.TensorDataset(inputs_tensor, targets_phoneme_tensor)\n",
    "\n",
    "# Define the split sizes (e.g., 70% train, 15% validation, 15% test)\n",
    "train_size = int(0.7 * len(full_dataset))\n",
    "val_size = int(0.15 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size - val_size\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset, val_dataset, test_dataset = random_split(full_dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Create DataLoaders for each set\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,num_workers=8)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,num_workers=8)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,num_workers=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "30d83ecc-0b49-4827-b12a-781aa7e338b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.2299, Train AUC: 0.9728, Val AUC: 0.9907, Duration: 233.77 seconds\n",
      "Epoch 2/10, Loss: 0.5170, Train AUC: 0.9947, Val AUC: 0.9969, Duration: 234.62 seconds\n",
      "Epoch 3/10, Loss: 0.3161, Train AUC: 0.9978, Val AUC: 0.9982, Duration: 1860.18 seconds\n",
      "Epoch 4/10, Loss: 0.2346, Train AUC: 0.9987, Val AUC: 0.9989, Duration: 217.29 seconds\n",
      "Epoch 5/10, Loss: 0.1933, Train AUC: 0.9991, Val AUC: 0.9991, Duration: 241.15 seconds\n",
      "Epoch 6/10, Loss: 0.1681, Train AUC: 0.9993, Val AUC: 0.9993, Duration: 218.49 seconds\n",
      "Epoch 7/10, Loss: 0.1511, Train AUC: 0.9994, Val AUC: 0.9994, Duration: 225.78 seconds\n",
      "Epoch 8/10, Loss: 0.1388, Train AUC: 0.9995, Val AUC: 0.9995, Duration: 218.30 seconds\n",
      "Epoch 9/10, Loss: 0.1290, Train AUC: 0.9996, Val AUC: 0.9995, Duration: 233.80 seconds\n",
      "Epoch 10/10, Loss: 0.1217, Train AUC: 0.9996, Val AUC: 0.9995, Duration: 229.86 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "\n",
    "train_auc_scores = []\n",
    "val_auc_scores = []\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    epoch_loss = 0.0\n",
    "    all_targets = []\n",
    "    all_probabilities = []\n",
    "\n",
    "    MFCC_model.train()\n",
    "    for batch_inputs, batch_targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = MFCC_model(batch_inputs)\n",
    "\n",
    "        output_reshaped = output.view(-1, output_dim)\n",
    "        batch_targets_reshaped = batch_targets.view(-1)\n",
    "        \n",
    "        loss = criterion(output_reshaped, batch_targets_reshaped)\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Collect all targets and probabilities for AUC calculation\n",
    "        probabilities = torch.nn.functional.softmax(output, dim=-1)\n",
    "        all_targets.extend(batch_targets_reshaped.cpu().numpy())\n",
    "        all_probabilities.extend(probabilities.view(-1, output_dim).cpu().detach().numpy())\n",
    "\n",
    "    # Compute AUC for the entire epoch\n",
    "    try:\n",
    "        train_auc = roc_auc_score(all_targets, all_probabilities, multi_class=\"ovr\", average=\"weighted\")\n",
    "    except ValueError:\n",
    "        train_auc = 0\n",
    "    train_auc_scores.append(train_auc)\n",
    "\n",
    "    # Validation Phase\n",
    "    MFCC_model.eval()\n",
    "    val_targets = []\n",
    "    val_probabilities = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_inputs, batch_targets in val_loader:\n",
    "            output = MFCC_model(batch_inputs)\n",
    "            probabilities = torch.nn.functional.softmax(output, dim=-1)\n",
    "            \n",
    "            val_targets.extend(batch_targets.view(-1).cpu().numpy())\n",
    "            val_probabilities.extend(probabilities.view(-1, output_dim).cpu().numpy())\n",
    "        \n",
    "    try:\n",
    "        val_auc = roc_auc_score(val_targets, val_probabilities, multi_class=\"ovr\", average=\"weighted\")\n",
    "    except ValueError:\n",
    "        val_auc = 0\n",
    "    val_auc_scores.append(val_auc)\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_duration = end_time - start_time\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss / len(train_loader):.4f}, Train AUC: {train_auc:.4f}, Val AUC: {val_auc:.4f}, Duration: {epoch_duration:.2f} seconds\")\n",
    "\n",
    "# Plot AUC Curves\n",
    "plt.plot(range(1, num_epochs + 1), train_auc_scores, label='Train AUC')\n",
    "plt.plot(range(1, num_epochs + 1), val_auc_scores, label='Validation AUC')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('AUC Score')\n",
    "plt.legend()\n",
    "plt.title('AUC Score per Epoch')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ebc3668f-9ce7-4714-b227-63ddcec5be0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.1365,F1 Score: 0.9635, Test AUC: 0.9995, Duration: 24.36 seconds\n",
      "Epoch 2/10, Loss: 0.1365,F1 Score: 0.9635, Test AUC: 0.9995, Duration: 24.62 seconds\n",
      "Epoch 3/10, Loss: 0.1365,F1 Score: 0.9635, Test AUC: 0.9995, Duration: 24.83 seconds\n",
      "Epoch 4/10, Loss: 0.1365,F1 Score: 0.9635, Test AUC: 0.9995, Duration: 25.30 seconds\n",
      "Epoch 5/10, Loss: 0.1365,F1 Score: 0.9635, Test AUC: 0.9995, Duration: 25.73 seconds\n",
      "Epoch 6/10, Loss: 0.1365,F1 Score: 0.9635, Test AUC: 0.9995, Duration: 27.70 seconds\n",
      "Epoch 7/10, Loss: 0.1365,F1 Score: 0.9635, Test AUC: 0.9995, Duration: 27.39 seconds\n",
      "Epoch 8/10, Loss: 0.1365,F1 Score: 0.9635, Test AUC: 0.9995, Duration: 27.75 seconds\n",
      "Epoch 9/10, Loss: 0.1365,F1 Score: 0.9635, Test AUC: 0.9995, Duration: 26.54 seconds\n",
      "Epoch 10/10, Loss: 0.1365,F1 Score: 0.9635, Test AUC: 0.9995, Duration: 26.10 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Initialize lists to store AUC scores per epoch\n",
    "test_auc_scores = []\n",
    "epoch_f1_scores = []\n",
    "epoch_losses = []\n",
    "\n",
    "# Number of testing epochs\n",
    "num_test_epochs = 10  # Adjust as needed\n",
    "\n",
    "for epoch in range(num_test_epochs):\n",
    "    start_time = time.time()\n",
    "    # Testing phase\n",
    "    with torch.no_grad():  # Disable gradient calculation for testing\n",
    "        all_targets = []\n",
    "        all_predictions = []\n",
    "        all_probabilities = []\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for batch_idx, (batch_inputs, batch_targets) in enumerate(test_loader):\n",
    "            output = MFCC_model(batch_inputs)  # Forward pass\n",
    "            \n",
    "            # Reshape for loss calculation\n",
    "            output_reshaped = output.view(-1, output_dim)\n",
    "            batch_targets_reshaped = batch_targets.view(-1)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(output_reshaped, batch_targets_reshaped)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Get predicted classes and probabilities for metrics\n",
    "            predictions = output.argmax(dim=-1)  # Class predictions\n",
    "            probabilities = F.softmax(output, dim=-1)  # Probability scores\n",
    "            \n",
    "            # Flatten and accumulate for F1 and AUC computation\n",
    "            all_targets.extend(batch_targets_reshaped.cpu().numpy())\n",
    "            all_predictions.extend(predictions.view(-1).cpu().numpy())\n",
    "            all_probabilities.extend(probabilities.view(-1, output_dim).cpu().numpy())\n",
    "\n",
    "        # Calculate metrics for the epoch\n",
    "        epoch_f1 = f1_score(all_targets, all_predictions, average=\"weighted\")\n",
    "        epoch_loss = total_loss / len(test_loader)\n",
    "        try:\n",
    "            test_auc = roc_auc_score(all_targets, all_probabilities, multi_class=\"ovr\", average=\"weighted\")\n",
    "        except ValueError:\n",
    "            test_auc = None  # Set AUC to None if not computable for this epoch\n",
    "\n",
    "        # Append metrics for plotting\n",
    "        epoch_f1_scores.append(epoch_f1)\n",
    "        epoch_losses.append(epoch_loss)\n",
    "        if test_auc is not None:\n",
    "            test_auc_scores.append(test_auc)\n",
    "        end_time = time.time()\n",
    "        epoch_duration = end_time - start_time\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f},F1 Score: {epoch_f1:.4f}, Test AUC: {test_auc:.4f}, Duration: {epoch_duration:.2f} seconds\")\n",
    "\n",
    "# Plot AUC scores per epoch\n",
    "plt.plot(range(1, len(test_auc_scores) + 1), test_auc_scores, label='Test AUC per Epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('AUC Score')\n",
    "plt.title('AUC Score Progression Across Epochs')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "309c8f6c-ca86-4c07-a958-19af5da994bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot AUC scores per epoch\n",
    "# Plot AUC Curves\n",
    "plt.plot(range(1, num_epochs + 1), train_auc_scores, label='Train AUC')\n",
    "plt.plot(range(1, num_epochs + 1), val_auc_scores, label='Validation AUC')\n",
    "plt.plot(range(1, len(test_auc_scores) + 1), test_auc_scores, label='Test AUC')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('AUC Score')\n",
    "plt.title('AUC Score Progression Across Epochs')\n",
    "plt.legend()\n",
    "plt.savefig(\"C:/Users/siddh/Downloads/AUC_MFCC_Phoneme_model.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82f478b-ae21-4589-9d16-14033b6264f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "\n",
    "# Initialize lists to store AUC scores and corresponding time\n",
    "batch_auc_scores = []\n",
    "batch_times = []\n",
    "\n",
    "# Testing phase\n",
    "with torch.no_grad():  # Disable gradient calculation for testing\n",
    "    all_targets = []\n",
    "    all_predictions = []\n",
    "    all_probabilities = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    start_time = time.time()  # Start timer for AUC over time\n",
    "\n",
    "    for batch_idx, (batch_inputs, batch_targets) in enumerate(test_loader):\n",
    "        output = MFCC_model(batch_inputs)  # Forward pass\n",
    "        \n",
    "        # Reshape for loss calculation\n",
    "        output_reshaped = output.view(-1, output_dim)\n",
    "        batch_targets_reshaped = batch_targets.view(-1)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(output_reshaped, batch_targets_reshaped)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Get predicted classes and probabilities for metrics\n",
    "        predictions = output.argmax(dim=-1)  # Class predictions\n",
    "        probabilities = F.softmax(output, dim=-1)  # Probability scores\n",
    "        \n",
    "        # Flatten and accumulate for F1 and AUC computation\n",
    "        all_targets.extend(batch_targets_reshaped.cpu().numpy())\n",
    "        all_predictions.extend(predictions.view(-1).cpu().numpy())\n",
    "        all_probabilities.extend(probabilities.view(-1, output_dim).detach().cpu().numpy())\n",
    "        \n",
    "        # Calculate cumulative AUC up to this batch\n",
    "        try:\n",
    "            auc = roc_auc_score(all_targets, all_probabilities, multi_class=\"ovr\", average=\"weighted\")\n",
    "        except ValueError:\n",
    "            auc = None  # Set AUC to None if not computable for this batch\n",
    "        if auc is not None:\n",
    "            batch_auc_scores.append(auc)  # Store AUC score\n",
    "            elapsed_time = time.time() - start_time  # Calculate time elapsed\n",
    "            batch_times.append(elapsed_time)  # Store time elapsed\n",
    "\n",
    "    # Final metrics calculation after the loop\n",
    "    f1 = f1_score(all_targets, all_predictions, average=\"weighted\")\n",
    "    test_auc = batch_auc_scores[-1] if batch_auc_scores else None\n",
    "    print(f\"Test Loss: {total_loss / len(test_loader):.4f}, F1 Score: {f1:.4f}, Final AUC: {test_auc}\")\n",
    "\n",
    "# Plot AUC curve over time\n",
    "plt.plot(batch_times, batch_auc_scores, label='Test AUC Over Time')\n",
    "plt.xlabel('Time (seconds)')\n",
    "plt.ylabel('AUC Score')\n",
    "plt.title('AUC Score Progression During Testing')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "54775fe1-a803-4d52-b415-b7938366069a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from textgrid import TextGrid\n",
    "\n",
    "def parse_textgrid_phoneme_to_word_sequence(textgrid_path):\n",
    "    # Initialize a list to hold phoneme-word sequence mappings\n",
    "    phoneme_word_mappings = []\n",
    "\n",
    "    # Load the TextGrid file\n",
    "    tg = TextGrid.fromFile(textgrid_path)\n",
    "\n",
    "    # Define the tiers you want to parse\n",
    "    tier_names = [\"phones\", \"words\"]\n",
    "\n",
    "    # Get intervals from phones and words tiers\n",
    "    phoneme_intervals = []\n",
    "    word_intervals = []\n",
    "\n",
    "    for tier_name in tier_names:\n",
    "        tier = tg.getFirst(tier_name)\n",
    "        if tier is None:\n",
    "            print(f\"Tier '{tier_name}' not found in {textgrid_path}.\")\n",
    "            return phoneme_word_mappings  # Return empty if any tier is missing\n",
    "\n",
    "        if tier_name == \"phones\":\n",
    "            phoneme_intervals = [(interval.minTime, interval.maxTime, interval.mark) for interval in tier.intervals if interval.mark]\n",
    "        elif tier_name == \"words\":\n",
    "            word_intervals = [(interval.minTime, interval.maxTime, interval.mark) for interval in tier.intervals if interval.mark]\n",
    "\n",
    "    # Create phoneme sequence mapping to each word based on overlapping intervals\n",
    "    phoneme_sequence = []\n",
    "    for word_start, word_end, word in word_intervals:\n",
    "        phoneme_sequence.clear()  # Clear phoneme sequence for each word\n",
    "        for phoneme_start, phoneme_end, phoneme in phoneme_intervals:\n",
    "            if phoneme_start >= word_start and phoneme_end <= word_end:\n",
    "                phoneme_sequence.append(phoneme)\n",
    "        \n",
    "        if phoneme_sequence:  # Only add non-empty mappings\n",
    "            phoneme_word_mappings.append((\" \".join(phoneme_sequence), word))\n",
    "\n",
    "    return phoneme_word_mappings\n",
    "\n",
    "def parse_all_textgrids_for_mappings(base_directory):\n",
    "    all_mappings = {}\n",
    "    \n",
    "    # Walk through all folders and files in the given base directory\n",
    "    for root, dirs, files in os.walk(base_directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".TextGrid\"):\n",
    "                textgrid_path = os.path.join(root, file)\n",
    "                # Parse the TextGrid file and get phoneme-to-word sequence mappings\n",
    "                mappings = parse_textgrid_phoneme_to_word_sequence(textgrid_path)\n",
    "                all_mappings[textgrid_path] = mappings\n",
    "\n",
    "    return all_mappings\n",
    "\n",
    "# Example usage\n",
    "base_directory = \"C:/Users/siddh/Documents/aligned_librispeech\"  # Correct assignment of base directory\n",
    "phoneme_word_mappings = parse_all_textgrids_for_mappings(base_directory)\n",
    "\n",
    "# Print or save the mappings as needed\n",
    "for tg_path, mappings in phoneme_word_mappings.items():\n",
    "    # Uncomment to print mappings\n",
    "    # print(f\"Mappings for {tg_path}:\")\n",
    "    for phoneme_sequence, word in mappings:\n",
    "        # Uncomment to print each mapping\n",
    "        # print(f\"{phoneme_sequence} -> {word}\")\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c88729ea-b30d-4245-8faf-c8974605b544",
   "metadata": {},
   "outputs": [],
   "source": [
    "#phoneme_word_mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0f39ae7f-b558-4aff-8a75-c8e2f8145983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize sets to store unique phonemes and words\n",
    "unique_phonemes = set()\n",
    "unique_words = set()\n",
    "\n",
    "# Loop through all mappings in the dataset\n",
    "for path, mapping in phoneme_word_mappings.items():\n",
    "    for phoneme, word in mapping:\n",
    "        unique_phonemes.add(phoneme)  # Add phoneme to set\n",
    "        unique_words.add(word)  # Add whole word to set\n",
    "\n",
    "# Convert sets to sorted lists for consistent ordering\n",
    "all_phonemes = sorted(unique_phonemes)\n",
    "all_words = sorted(unique_words)\n",
    "\n",
    "# Create word to index mapping\n",
    "word_to_index = {word: idx + 1 for idx, word in enumerate(all_words,start = 1)}  # Start from 1\n",
    "word_to_index['<pad>'] = 0  # Add padding index if needed\n",
    "\n",
    "\n",
    "# Create index to word mapping for easy lookup later\n",
    "index_to_word = {idx: word for word, idx in word_to_index.items()}\n",
    "\n",
    "# Create phoneme to index mapping\n",
    "# Add a special `<unk>` token and `<pad>` token to `phoneme_to_index` if not already there\n",
    "phoneme_to_index = {p: i for i, p in enumerate(all_phonemes, start=1)}  # Shift indices to start from 1\n",
    "phoneme_to_index['<unk>'] = len(phoneme_to_index) + 1  # Unknown phoneme\n",
    "phoneme_to_index['<pad>'] = 0  # Padding token\n",
    "\n",
    "\n",
    "# Create index to phoneme mapping for easy lookup later\n",
    "index_to_phoneme = {idx: phoneme for phoneme, idx in phoneme_to_index.items()}\n",
    "\n",
    "# Printing mappings to verify\n",
    "#print(\"All unique phonemes:\", all_phonemes)\n",
    "#print(\"All unique words:\", all_words)\n",
    "#print(\"\\nWord to Index Mapping:\", word_to_index.values())\n",
    "#print(\"\\nIndex to Word Mapping:\", index_to_word)\n",
    "#print(\"\\nPhoneme to Index Mapping:\", phoneme_to_index)\n",
    "#print(\"\\nIndex to Phoneme Mapping:\", index_to_phoneme)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "0117f3d3-20ff-42ec-88fc-48b0103085d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2412\n",
      "2358\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class PhonemeToWordLSTM(nn.Module):\n",
    "    def __init__(self, phoneme_vocab_size, word_vocab_size, embedding_dim=64, hidden_dim=128, num_layers=2):\n",
    "        super(PhonemeToWordLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(phoneme_vocab_size, embedding_dim,padding_idx=phoneme_to_index['<pad>'])\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, word_vocab_size)\n",
    "        \n",
    "    def forward(self, phoneme_sequence):\n",
    "        # Embed the phoneme sequence\n",
    "        embedded = self.embedding(phoneme_sequence)  # Shape: (batch_size, seq_len, embedding_dim)\n",
    "        \n",
    "        # Pass through LSTM\n",
    "        lstm_out, _ = self.lstm(embedded)  # Shape: (batch_size, seq_len, hidden_dim)\n",
    "        \n",
    "        # Use only the last LSTM output (for sequence-to-word task)\n",
    "        final_output = lstm_out[:, -1, :]  # Shape: (batch_size, hidden_dim)\n",
    "        \n",
    "        # Pass through fully connected layer to predict the word\n",
    "        word_prediction = self.fc(final_output)  # Shape: (batch_size, word_vocab_size)\n",
    "        \n",
    "        return word_prediction\n",
    "\n",
    "\n",
    "\n",
    "# Example of model usage\n",
    "phoneme_vocab_size = len(all_phonemes) + 2  # +2 for <pad> and <unk> tokens\n",
    "word_vocab_size = len(all_words) + 2\n",
    "print(phoneme_vocab_size)\n",
    "print(word_vocab_size)\n",
    "\n",
    "Phoneme_Word_LSTM = PhonemeToWordLSTM(phoneme_vocab_size, word_vocab_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "fe29e50d-8afd-4fbd-b11f-cafb4d094c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(2412, 64, padding_idx=0)\n"
     ]
    }
   ],
   "source": [
    "print(Phoneme_Word_LSTM.embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "70dfc6cc-d2de-458c-b8ed-43f141fcf453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2410\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<unk>'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(all_phonemes))\n",
    "index_to_phoneme[max(index_to_phoneme.keys())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "ad746984-6614-45d0-aac9-f56d66eda805",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhonemeWordDataset(Dataset):\n",
    "    def __init__(self, data, phoneme_to_index, word_to_index):\n",
    "        self.data = data\n",
    "        self.phoneme_to_index = phoneme_to_index\n",
    "        self.word_to_index = word_to_index\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        phoneme_word_data = self.data[idx]\n",
    "        audio_path, phoneme_word_pairs = phoneme_word_data\n",
    "\n",
    "        # Convert phoneme and word sequences to index sequences\n",
    "        phoneme_targets = [\n",
    "            self.phoneme_to_index.get(phoneme, self.phoneme_to_index['<unk>'])\n",
    "            for phoneme, _ in phoneme_word_pairs\n",
    "        ]\n",
    "        word_targets = [\n",
    "            self.word_to_index.get(word, self.word_to_index.get('<unk>', -1))\n",
    "            for _, word in phoneme_word_pairs\n",
    "        ]\n",
    "\n",
    "        # Convert lists of indices to tensors\n",
    "        phoneme_tensor = torch.tensor(phoneme_targets)\n",
    "        word_tensor = torch.tensor(word_targets)\n",
    "\n",
    "        return phoneme_tensor, word_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "febe2584-1dd0-4bcc-b0bf-1bf871668fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Parameters for split sizes\n",
    "train_ratio = 0.8\n",
    "val_ratio = 0.1\n",
    "test_ratio = 0.1\n",
    "\n",
    "# Step 1: Convert phoneme_word_mappings dictionary to lists\n",
    "data = [(textgrid_path, phonemes_word) for texgrid_path, phonemes_word in phoneme_word_mappings.items()]\n",
    "\n",
    "\n",
    "# Step 2: Create train, validation, and test splits\n",
    "train_size = int(train_ratio * len(data))\n",
    "val_size = int(val_ratio * len(data))\n",
    "test_size = len(data) - train_size - val_size\n",
    "\n",
    "train_data, val_data, test_data = random_split(data, [train_size, val_size, test_size])\n",
    "\n",
    "# Step 5: Create Dataset instances\n",
    "train_dataset = PhonemeWordDataset(train_data, phoneme_to_index, word_to_index)\n",
    "val_dataset = PhonemeWordDataset(val_data, phoneme_to_index, word_to_index)\n",
    "test_dataset = PhonemeWordDataset(test_data, phoneme_to_index, word_to_index)\n",
    "\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "\n",
    "def custom_collate_fn(batch, pad_idx):\n",
    "    # Extract phoneme and word tensors from the batch\n",
    "    phoneme_tensors = [torch.tensor(phonemes, dtype=torch.long) for phonemes, _ in batch]\n",
    "    word_tensors = [torch.tensor(words, dtype=torch.long) for _, words in batch]\n",
    "\n",
    "    # Padding the phoneme sequences\n",
    "    phoneme_padded = torch.nn.utils.rnn.pad_sequence(phoneme_tensors, batch_first=True, padding_value=pad_idx)\n",
    "    \n",
    "    # Padding the word sequences\n",
    "    max_word_length = max([wt.size(0) for wt in word_tensors])\n",
    "    word_padded = torch.stack([torch.cat([wt, torch.tensor([pad_idx] * (max_word_length - wt.size(0)), dtype=torch.long)]) for wt in word_tensors], dim=0)\n",
    "    \n",
    "    return phoneme_padded, word_padded\n",
    "\n",
    "\n",
    "\n",
    "# Step 6: Create DataLoaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=lambda x: custom_collate_fn(x, pad_idx=0))\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=lambda x: custom_collate_fn(x, pad_idx=0))\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=lambda x: custom_collate_fn(x, pad_idx=0))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "656c2eca-db0e-443c-95c1-1a504296736b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0 - Phonemes: tensor([1072, 1119,  325, 2160,  337,  542,  860, 1742, 2410, 1136,   17, 2394,\n",
      "         542, 2209,  895, 1845, 1495, 1683,  838,  987,  897,  439,   76,  987,\n",
      "         542, 1787,  239, 1414,   65,  897, 1810,   76, 1853,  702, 1163]), Word index: tensor([1130,  888,  225, 2130,  205, 2078,  999, 1613,  656,  298,   82, 1146,\n",
      "        2078, 2155,  988, 1712, 1347, 1620,  949, 1064, 1007,  498,  101, 1064,\n",
      "        2078, 1708, 1442, 1272,   61, 1007, 1727,  101, 1760,  869,  313])\n",
      "Sample 1 - Phonemes: tensor([ 932, 2318, 1219,   76,  987, 2364, 2313,  570,  253, 1432,  938,  987,\n",
      "         542, 2352,   76,  563,  231, 2318,  542, 1721,  226, 1432, 1343,  987,\n",
      "        2064,  518,  105, 1178,  608,  274, 1943,  184, 2313, 1238,  181,  547,\n",
      "         911, 2029,   76, 2364]), Word index: tensor([2303, 2270,  435,  101, 1064, 2303, 2270,  657,   72, 1366,  643, 1064,\n",
      "        2078, 2337,  101, 2078, 1477, 2270, 2078, 1603, 1442, 1366, 1222, 1064,\n",
      "        1928,  582,    2,  295,  153, 1041, 1822,  123, 2270,  383, 2217, 2078,\n",
      "        2309,  362,  101, 2303])\n",
      "Sample 2 - Phonemes: tensor([  76,  987,  542, 1717,  226,   15,  542, 1662,  253,  541,  906,  830,\n",
      "        1957,  341,   87,  296,  822]), Word index: tensor([ 101, 1064, 2078, 1601, 1442,   72, 2078, 1531,   72, 2077,  963,  939,\n",
      "        1808,  223,  158,  294, 1013])\n",
      "Sample 3 - Phonemes: tensor([  88,  542, 2001, 2174,  987, 1332,   76, 1505, 1443, 2392,  344, 1237,\n",
      "        1577,  702, 1619,   76,   46, 2361,  902,  905,   78, 2318,  987, 2266,\n",
      "        2348]), Word index: tensor([ 170, 2078, 1814, 2127, 1064, 1238,  101, 1412, 1277, 2348,  215,  382,\n",
      "        1483,  869, 1578,  101,   54, 2326, 1002, 1007,  110, 2270, 1064, 2116,\n",
      "        2334])\n",
      "Sample 4 - Phonemes: tensor([ 541,  542, 1751,  342,  932, 2378, 2223,  181,  987, 1245, 1934,  732,\n",
      "         625, 1600,   90,  807,   90, 2338,  105, 1115,  423,   76,  541,  105,\n",
      "        2388, 2386,  987, 1333,   18, 1390]), Word index: tensor([2077, 2078, 1574,  213, 2303, 2277, 2160, 2217, 1064,  460, 2016,  785,\n",
      "           2, 1497,  153,  922,  153, 2294,    2,  895,  563,  101, 2077,    2,\n",
      "        2351, 2330, 1064, 1252,   86, 1245])\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    phoneme_tensor, word_tensor = train_dataset[i]\n",
    "    #phoneme_tensors = [torch.tensor(phonemes, dtype=torch.long).clone().detach() for phonemes, _ in batch]\n",
    "    # = [torch.tensor(words, dtype=torch.long).clone().detach() for _, words in batch]\n",
    "\n",
    "    print(f\"Sample {i} - Phonemes: {phoneme_tensor}, Word index: {word_tensor}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "57c1d259-e41e-488a-b4b4-a24b09ff1557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch inputs shape: torch.Size([32, 43])\n",
      "Batch targets shape: torch.Size([32, 43])\n",
      "tensor([[ 290,  897, 1059,  ...,    0,    0,    0],\n",
      "        [ 830,  334,  563,  ...,    0,    0,    0],\n",
      "        [2361,  105, 1842,  ...,  105, 1478,  955],\n",
      "        ...,\n",
      "        [ 274, 1385, 1044,  ...,    0,    0,    0],\n",
      "        [2064,  765,  321,  ...,    0,    0,    0],\n",
      "        [ 541,  563, 1002,  ...,    0,    0,    0]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\AppData\\Local\\Temp\\ipykernel_21296\\1999161309.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  phoneme_tensors = [torch.tensor(phonemes, dtype=torch.long) for phonemes, _ in batch]\n",
      "C:\\Users\\siddh\\AppData\\Local\\Temp\\ipykernel_21296\\1999161309.py:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  word_tensors = [torch.tensor(words, dtype=torch.long) for _, words in batch]\n"
     ]
    }
   ],
   "source": [
    "for batch_inputs, batch_targets in train_loader:\n",
    "    print(\"Batch inputs shape:\", batch_inputs.shape)  # [batch_size, max_seq_len]\n",
    "    print(\"Batch targets shape:\", batch_targets.shape)  # [batch_size]\n",
    "    \n",
    "    break\n",
    "print(batch_inputs)\n",
    "#print(batch_targets, min(batch_targets),max(batch_targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "0d81fcbc-cbd6-4c89-8a17-7f4bd8b431a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample phoneme sequence: ['AA0 L R EH1 D IY0']\n",
      "Predicted word: involuntary\n"
     ]
    }
   ],
   "source": [
    "# Assume phoneme_to_index, PhonemeWord LSTM model, and device (e.g., 'cpu' or 'cuda') are already defined.\n",
    "\n",
    "# Step 1: Define a sample phoneme sequence\n",
    "sample_phoneme_sequence = ['AA0 L R EH1 D IY0']  # Example sequence\n",
    "sample_phoneme_indices = [phoneme_to_index[p] for p in sample_phoneme_sequence]\n",
    "\n",
    "# Step 2: Convert to tensor and add batch dimension\n",
    "sample_phoneme_tensor = torch.tensor(sample_phoneme_indices, dtype=torch.long).unsqueeze(0)  # Shape: [1, sequence_length]\n",
    "\n",
    "# Step 3: Set the model to evaluation mode (no backpropagation required)\n",
    "Phoneme_Word_LSTM.eval()\n",
    "\n",
    "# Step 4: Feed the sample tensor into the model\n",
    "with torch.no_grad():  # Disable gradient calculation for inference\n",
    "    output = Phoneme_Word_LSTM(sample_phoneme_tensor)\n",
    "\n",
    "# Step 5: Process the output (e.g., predicted word index)\n",
    "# If the model outputs a sequence of logits or probabilities, use argmax to find the predicted indices\n",
    "predicted_index = output.argmax(dim=-1).item()\n",
    "\n",
    "# Convert predicted index back to a word\n",
    "predicted_word = index_to_word[predicted_index]\n",
    "\n",
    "# Print the result\n",
    "print(\"Sample phoneme sequence:\", sample_phoneme_sequence)\n",
    "print(\"Predicted word:\", predicted_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "511b2b56-155b-4e01-8c71-6e7696ac1f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\AppData\\Local\\Temp\\ipykernel_21296\\1999161309.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  phoneme_tensors = [torch.tensor(phonemes, dtype=torch.long) for phonemes, _ in batch]\n",
      "C:\\Users\\siddh\\AppData\\Local\\Temp\\ipykernel_21296\\1999161309.py:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  word_tensors = [torch.tensor(words, dtype=torch.long) for _, words in batch]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (32) to match target batch_size (1504).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[158], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(batch_inputs)  \u001b[38;5;66;03m# Outputs should be [batch_size, seq_len, num_classes]\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Calculate loss\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_targets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:1293\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1292\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1294\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1297\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1298\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1299\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1300\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\functional.py:3479\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3478\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3479\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3480\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3481\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3482\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3483\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3484\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3485\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3486\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected input batch_size (32) to match target batch_size (1504)."
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for batch_inputs, batch_targets in train_loader:\n",
    "        # Move data to the device (GPU/CPU)\n",
    "        batch_inputs, batch_targets = batch_inputs.to(device), batch_targets.to(device)\n",
    "\n",
    "        # Flatten batch_targets if necessary (for sequence classification)\n",
    "        batch_targets = batch_targets.view(-1)  # Flatten to [batch_size * seq_len]\n",
    "\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(batch_inputs)  # Outputs should be [batch_size, seq_len, num_classes]\n",
    "        # Assuming `output` is your model's output of shape (batch_size, seq_length, num_classes)\n",
    "        # and `target` is of shape (batch_size, seq_length)\n",
    "\n",
    "        output_flat = output.view(-1, output.shape[-1])  # Shape: (batch_size * seq_length, num_classes)\n",
    "\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, batch_targets)\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Convert model outputs to predicted labels (for metrics calculation)\n",
    "        preds = torch.argmax(outputs, dim=2).cpu().numpy().flatten()  # Take the argmax for each word in the sequence\n",
    "        labels = batch_targets.cpu().numpy().flatten()\n",
    "\n",
    "        # Store predictions and true labels for metrics calculation\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labels)\n",
    "\n",
    "    # Calculate F1 and AUC for the epoch\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    auc = roc_auc_score(all_labels, all_preds, average='weighted', multi_class='ovr')\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss/len(train_loader):.4f}, F1 Score: {f1:.4f}, AUC: {auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "79ab4d1e-0a4c-4690-a0a5-e5c25dd70154",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\AppData\\Local\\Temp\\ipykernel_21296\\1999161309.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  phoneme_tensors = [torch.tensor(phonemes, dtype=torch.long) for phonemes, _ in batch]\n",
      "C:\\Users\\siddh\\AppData\\Local\\Temp\\ipykernel_21296\\1999161309.py:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  word_tensors = [torch.tensor(words, dtype=torch.long) for _, words in batch]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (32) to match target batch_size (1568).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[160], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m batch_targets_flat \u001b[38;5;241m=\u001b[39m batch_targets\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Calculate the loss\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_targets_flat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     25\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:1293\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1292\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1294\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1297\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1298\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1299\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1300\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\functional.py:3479\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3478\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3479\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3480\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3481\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3482\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3483\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3484\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3485\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3486\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected input batch_size (32) to match target batch_size (1568)."
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for batch_inputs, batch_targets in train_loader:\n",
    "        # Move data to the device (GPU/CPU)\n",
    "        batch_inputs, batch_targets = batch_inputs.to(device), batch_targets.to(device)\n",
    "\n",
    "        # Flatten batch_targets if necessary (for multi-label sequence)\n",
    "        batch_targets = batch_targets.view(-1)\n",
    "\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(batch_inputs)  # Get model outputs\n",
    "\n",
    "        # Flatten the batch_targets (word tensor) for loss calculation\n",
    "        batch_targets_flat = batch_targets.flatten()\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = criterion(outputs.view(-1, outputs.size(-1)), batch_targets_flat)\n",
    "        epoch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Convert model outputs to predicted labels\n",
    "        preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "        labels = batch_targets.cpu().numpy()\n",
    "\n",
    "        # Store predictions and true labels for metrics calculation\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labels)\n",
    "\n",
    "    # Calculate F1 and AUC for the epoch\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    auc = roc_auc_score(all_labels, all_preds, average='weighted', multi_class='ovr')\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss/len(train_loader):.4f}, F1 Score: {f1:.4f}, AUC: {auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f3c19d-5ed1-4c39-95eb-ae74a7de5ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_pred_probs = []  # Store full probabilities for AUC calculation\n",
    "    \n",
    "    for phoneme_batch, word_batch in dataloader:\n",
    "        phoneme_batch, word_batch = phoneme_batch.to(device), word_batch.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(phoneme_batch)  # Shape: (batch_size, word_vocab_size)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(output, word_batch)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Get probabilities (using softmax to get a distribution)\n",
    "        pred_probs = torch.softmax(output, dim=1)  # Apply softmax to get probabilities\n",
    "        \n",
    "        # Get predictions (class with highest probability)\n",
    "        _, preds = torch.max(pred_probs, dim=1)  # Get predicted class indices\n",
    "\n",
    "        print(pred_probs.shape, preds.shape, word_batch.shape)\n",
    "        \n",
    "        # Accumulate predictions and labels for AUC and F1 calculation\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(word_batch.cpu().numpy())\n",
    "        \n",
    "        # Store full probabilities for AUC calculation (no need to gather, store entire prediction set)\n",
    "        all_pred_probs.extend(pred_probs.cpu().detach().numpy())\n",
    "    \n",
    "    # Debug: Check number of unique classes in y_true and y_pred\n",
    "    num_classes_true = len(np.unique(all_labels))\n",
    "    num_classes_pred = all_pred_probs[0].shape[0] if all_pred_probs else 0\n",
    "    print(f\"Number of classes in y_true: {num_classes_true}, number of classes in y_pred: {num_classes_pred}\")\n",
    "    \n",
    "    # Ensure the shapes are consistent before calculating AUC\n",
    "    if num_classes_true != num_classes_pred:\n",
    "        raise ValueError(f\"Mismatch between the number of classes: {num_classes_true} != {num_classes_pred}\")\n",
    "    \n",
    "    # Calculate AUC (macro average across all classes)\n",
    "    auc = roc_auc_score(all_labels, all_pred_probs, average='macro', multi_class='ovr')\n",
    "    \n",
    "    # Calculate F1 score (for predicted classes vs true labels)\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "    return total_loss / len(dataloader), auc, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "df47cc2f-1efa-418c-9ed2-b9843b4aae46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  72, 2078,  364, 1026, 2130, 2078, 1369, 1442,  303, 2290, 1863, 1064,\n",
      "        1350, 1147, 1362, 1278, 1442,  171])\n",
      "tensor([   9, 2351,  898, 2077,  960,  212, 1962,  834, 1015,  294,  922,   32,\n",
      "        2078, 1321, 1041, 2097, 1041, 2317, 1241, 1136, 1481,  834, 2350, 1935,\n",
      "         497, 1466, 1477,  209, 1041,   87, 2030, 1136, 2317,  600, 2350,  910,\n",
      "         320, 1808, 1401, 1341,  101, 2326,   99,  670, 2130,  600, 1777])\n",
      "tensor([ 869, 2078,  565, 1559, 2299, 2091, 2290, 1827, 2091,  140,  382, 2132])\n",
      "tensor([2308,   82,  385, 2083, 2130,  528,  869, 2086,  101, 2130,  624, 1064,\n",
      "         340,  101, 2294, 2087, 2270,    2,  764, 1483,   72, 2078, 1180, 2091,\n",
      "        2289,  611, 1119,  645,  101, 2087,  834,  853, 1036, 2348, 2290, 1928,\n",
      "        1357])\n",
      "tensor([2078, 1654,  140,  688, 2130,  206, 1064, 1317, 2077, 2100, 2348,  960,\n",
      "        1517, 1905, 1136, 2270,  807, 1277, 1341, 1905, 1136, 2270,  219,  101,\n",
      "        2077,  621, 2077, 1540, 1559, 1275,  257,  101, 1460,  960, 2194,  435,\n",
      "         337])\n",
      "tensor([2308, 2290, 1483,   65, 2078,  752, 1442, 2078,  535])\n",
      "tensor([ 153, 2078,  323, 1442, 1007, 1122,  963,  560, 1412,   42, 1003, 2130,\n",
      "          99, 2190, 1149, 1466,    2, 1736,  971,  765,  869,  403, 1002, 1466,\n",
      "        1007, 1910, 1064, 2081,  770, 1324])\n",
      "tensor([1412, 1304, 2130,  604,  292, 2130, 2078, 1341,  990,  679, 1442, 1086,\n",
      "        1423,    2,  607,  781,    2,  307,  243, 1466, 2273,    2, 1789,  290,\n",
      "        1077, 1878,  939, 1401, 2056,  834,    2,  881])\n",
      "tensor([2085, 2108,   82, 1876,  561, 2326, 2083,  294, 2078, 2042, 1442, 2078,\n",
      "         168])\n",
      "tensor([ 101, 1045, 1878,  882,  825,  170,   72, 1136, 2270,  349,  834, 2078,\n",
      "        1567, 1442, 1323,  170, 1202, 1928, 1136, 2270,  424,  869,  982,   86,\n",
      "        1596, 2107, 2303, 1878, 2270,  835, 2130, 2049, 2012, 2290,  982, 1636])\n",
      "tensor([1213, 2220,  905, 2216, 1119, 2078, 1351,  101, 2294, 2078,  269, 1442,\n",
      "        2083, 1875,  205, 2050])\n",
      "tensor([2326,    2, 1241, 1442, 1356, 1748,  963, 1053, 1789,  101,  223, 1120,\n",
      "        2130,  982,  294,  982,  431,  495,  153, 1332,  980, 2125, 2326, 2078,\n",
      "         658, 1442, 1684, 1853])\n",
      "tensor([2298, 2078,  700, 1466,  356, 1302, 2308, 1161, 2084, 1757,  101, 1739,\n",
      "         888,  695, 2078,  442, 2217, 2090,  142, 1366,  284,  336, 1366, 1744,\n",
      "          64])\n",
      "tensor([ 939,   17, 1099, 1442, 2105, 1045,  963, 1173, 1341, 1442,  982, 2073,\n",
      "         982, 1369, 2105, 1347,  950, 2130,  205, 1451, 1952, 2067, 2326,    2,\n",
      "        1271, 1442,  885, 2126, 1056,  939,  212, 1148,  101, 1643,  393])\n",
      "tensor([ 375,  802, 2326, 1531, 1442, 2078,  237, 1942,  101, 1023,  179, 2083,\n",
      "         101,   72, 1040,  193, 2085,  663, 2256,  101,  317,  127,  101, 1950,\n",
      "        1024, 2326, 1860, 2308, 1136, 2270,  122])\n",
      "tensor([2326,   72, 1652, 2018,  834, 2078,  147, 1064,   15,  101,  983, 1442,\n",
      "        1493,  101,  823, 1442, 1879])\n",
      "tensor([1774, 2270,  167, 1442, 2078, 2025, 1442,  199,  628, 1442, 2311,  463,\n",
      "         277,   99,  149, 1442,  791, 1756, 1119, 2078,  794])\n",
      "tensor([  61,    2,  470, 1442, 1320, 2189, 1902,  980, 2178, 2130,  320,  834,\n",
      "        2078,  810, 2127, 1905,  982, 1349,  687,  158,  982, 2326, 2013,   71,\n",
      "        1045, 1332,  101, 1330,   73, 2290, 1419,  170,  874])\n",
      "tensor([1064, 2078, 1338, 1442, 1154,  101, 2289, 2130, 1494, 2297, 1136, 1566,\n",
      "         907, 2130, 1604, 2083,  101, 2078,  593,  560, 1412,  153, 1041,  969,\n",
      "        1442, 1928, 1356,  153, 2143, 2083,  834, 2303, 1041,  309, 1823, 2077])\n",
      "tensor([2091, 2340,  905, 2191, 1119, 1087, 1559,  101,  454, 2326, 1087, 1547,\n",
      "         294, 2303, 1290, 2091,  562,  170, 2078, 1677, 1442, 2065, 1466,  796,\n",
      "        2113,    2, 2283])\n",
      "tensor([ 529,  869, 2078,  471, 2303, 2091, 1472, 1433, 1064, 2078, 2154, 1442,\n",
      "         291, 1451, 2078, 1814,  497, 1451, 2303,  963, 1260, 1007,  128, 2130,\n",
      "        2083,   99, 1469,  614, 1064,  713,  420, 2326, 1007, 1734])\n",
      "tensor([ 101, 2078, 1560,  939,  327,  869, 2083,  101, 2091,  939,    2, 1317,\n",
      "        2130, 2049, 2083,   64])\n",
      "tensor([ 858,  294,  982, 1348,  982, 1627, 1064,  650, 2270, 1412, 1720,  101,\n",
      "        1878, 1884,  982, 1211, 1064,  263, 2296, 1878,  463, 2292,    2, 1994,\n",
      "        2186,  339,  834, 2326,   72, 2090, 2045, 1442, 1629,  170, 2065, 2348,\n",
      "        1450])\n",
      "tensor([ 222,  869, 2103, 1027, 2108, 1876,  205,  165, 2130, 2081, 1531, 2077,\n",
      "        2294, 2091, 1875, 1690, 2078, 1659, 2091,  536,  869, 1366, 2042])\n",
      "tensor([2326,    2, 1734, 2077, 2078, 2038, 1883, 1313,  205,  552, 2078, 1261,\n",
      "        1442,   98,  111, 2077, 2078,  687, 1442, 2078, 2357, 2355, 2270, 1481,\n",
      "        1442, 2081, 1155,  101, 1695, 1002, 2130, 2078,  780,  916, 1136, 2270,\n",
      "        1417, 2077, 2103, 2270,    2, 1303,  726])\n",
      "tensor([  72, 2078, 1531,  786, 2217, 2081,  753,   49, 2078, 1246,  101,   72,\n",
      "        1442, 2083, 2132, 1352,  101, 2286])\n",
      "tensor([ 963,  549, 2081,  360,  101, 2081,  908])\n",
      "tensor([2185, 1552,  101,    2, 2032, 2290, 1470, 2130,  905, 2130, 2078, 1029,\n",
      "         101, 1265, 1102, 2103, 2091,  560,  101,  804,  709, 2133, 1442, 2078,\n",
      "        1898, 2217,  263, 2078,  253, 2077, 2290,  499])\n",
      "tensor([2078,  477, 1442,    2, 2315, 1051, 2317,  170, 1202,  205,   72, 1366,\n",
      "        1490,    2, 2234, 1889, 2245, 2130, 1330,   73, 1064, 2303,  980, 2053,\n",
      "         170, 1673, 2328, 1852, 1466,  427,  101,  320])\n",
      "tensor([ 101,  963,  837,   72, 2078, 1974,  360, 2077, 2290, 2087,  869, 2078,\n",
      "        2141, 1442, 1270])\n",
      "tensor([ 959,  417,  101,  369,  101, 1253, 1414, 1928, 2288, 1064, 2078, 2337,\n",
      "         153, 1783,  611, 2078,  926, 1922,  170, 2078,  195, 1442, 2078, 1029,\n",
      "        2012, 2270,  320, 1342,  170, 2065,  170,  796,  126, 2290, 1298])\n",
      "tensor([ 101, 1442, 2078,  997,  101, 1442, 2078,  998,  101, 1442, 2078,   96,\n",
      "         101,   72, 2078, 1314, 1453, 1064,  996])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\AppData\\Local\\Temp\\ipykernel_21296\\2266502013.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  phoneme_tensors = [torch.tensor(phonemes, dtype=torch.long) for phonemes, _ in batch]\n",
      "C:\\Users\\siddh\\AppData\\Local\\Temp\\ipykernel_21296\\2266502013.py:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  word_tensors = [torch.tensor(words, dtype=torch.long) for _, words in batch]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "0D or 1D target tensor expected, multi-target not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[142], line 188\u001b[0m\n\u001b[0;32m    185\u001b[0m criterion \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m    187\u001b[0m \u001b[38;5;66;03m# Assuming train_loader and val_loader are defined\u001b[39;00m\n\u001b[1;32m--> 188\u001b[0m train_losses, val_losses, train_auc_scores, val_auc_scores, train_f1_scores, val_f1_scores \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\n\u001b[0;32m    190\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;66;03m# Plot AUC Curve\u001b[39;00m\n\u001b[0;32m    193\u001b[0m plot_auc_curve(train_auc_scores, val_auc_scores)\n",
      "Cell \u001b[1;32mIn[142], line 45\u001b[0m, in \u001b[0;36mtrain_and_validate\u001b[1;34m(model, train_loader, val_loader, optimizer, criterion, epochs)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[0;32m     44\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m---> 45\u001b[0m     train_loss, train_auc, train_f1 \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n\u001b[0;32m     47\u001b[0m     train_auc_scores\u001b[38;5;241m.\u001b[39mappend(train_auc)\n",
      "Cell \u001b[1;32mIn[142], line 82\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(model, dataloader, optimizer, criterion)\u001b[0m\n\u001b[0;32m     79\u001b[0m output \u001b[38;5;241m=\u001b[39m model(phoneme_batch)  \u001b[38;5;66;03m# Shape: (batch_size, word_vocab_size)\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[1;32m---> 82\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     83\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m# Get probabilities (using softmax to get a distribution)\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:1293\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1292\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1294\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1297\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1298\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1299\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1300\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\functional.py:3479\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3478\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3479\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3480\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3481\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3482\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3483\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3484\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3485\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3486\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: 0D or 1D target tensor expected, multi-target not supported"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Example Check: Verify phoneme indices and embedding size\n",
    "def check_phoneme_indices(phoneme_batch, model):\n",
    "    max_phoneme_idx = phoneme_batch.max().item()\n",
    "    embedding_size = model.embedding.num_embeddings\n",
    "    print(f\"Max phoneme index: {max_phoneme_idx}, Embedding size: {embedding_size}\")\n",
    "    if max_phoneme_idx >= embedding_size:\n",
    "        print(f\"Warning: Phoneme index {max_phoneme_idx} exceeds embedding size {embedding_size}\")\n",
    "    else:\n",
    "        print(f\"Phoneme indices are within the embedding size range.\")\n",
    "\n",
    "\n",
    "def find_out_of_bounds_indices(phoneme_batch, embedding_size):\n",
    "    \"\"\"\n",
    "    Identifies phoneme indices in phoneme_batch that exceed the embedding layer's size.\n",
    "\n",
    "    Args:\n",
    "        phoneme_batch (torch.Tensor): Batch of phoneme indices.\n",
    "        embedding_size (int): Size of the embedding layer.\n",
    "\n",
    "    Returns:\n",
    "        list: List of tuples with (batch_index, phoneme_index, phoneme_value) that are out of bounds.\n",
    "    \"\"\"\n",
    "    out_of_bounds_indices = []\n",
    "    for batch_index, phoneme_idx in enumerate(phoneme_batch.flatten()):\n",
    "        if phoneme_idx.item() >= embedding_size:\n",
    "            out_of_bounds_indices.append((batch_index, phoneme_idx.item()))\n",
    "    return out_of_bounds_indices\n",
    "\n",
    "# Training and validation loop with AUC, F1 score calculation\n",
    "def train_and_validate(model, train_loader, val_loader, optimizer, criterion, epochs):\n",
    "    train_losses, val_losses = [], []\n",
    "    train_auc_scores, val_auc_scores = [], []\n",
    "    train_f1_scores, val_f1_scores = [], []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss, train_auc, train_f1 = train_epoch(model, train_loader, optimizer, criterion)\n",
    "        train_losses.append(train_loss)\n",
    "        train_auc_scores.append(train_auc)\n",
    "        train_f1_scores.append(train_f1)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss, val_auc, val_f1 = validate_epoch(model, val_loader, criterion)\n",
    "        val_losses.append(val_loss)\n",
    "        val_auc_scores.append(val_auc)\n",
    "        val_f1_scores.append(val_f1)\n",
    "        \n",
    "        # Print Epoch Results\n",
    "        print(f\"Epoch {epoch+1}/{epochs}: \"\n",
    "              f\"Train Loss = {train_loss:.4f}, Train AUC = {train_auc:.4f}, Train F1 = {train_f1:.4f}, \"\n",
    "              f\"Val Loss = {val_loss:.4f},  Val F1 = {val_f1:.4f}\") #Val AUC = {val_auc:.4f},\n",
    "    \n",
    "    return train_losses, val_losses, train_auc_scores, train_f1_scores, val_f1_scores#val_auc_scores\n",
    "\n",
    "\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_pred_probs = []  # Store probabilities for AUC calculation\n",
    "    \n",
    "    for phoneme_batch, word_batch in dataloader:\n",
    "        phoneme_batch, word_batch = phoneme_batch.to(device), word_batch.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(phoneme_batch)  # Shape: (batch_size, word_vocab_size)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(output, word_batch)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Get probabilities (using softmax to get a distribution)\n",
    "        pred_probs = torch.softmax(output, dim=1)  # Apply softmax to get probabilities\n",
    "        \n",
    "        # Extract relevant probabilities for AUC and F1 (based on true label indices)\n",
    "        batch_size = word_batch.size(0)\n",
    "        relevant_pred_probs = pred_probs.gather(1, word_batch.view(-1, 1))  # Shape: (batch_size, 1)\n",
    "        \n",
    "        # Convert relevant_pred_probs to a 1D array for easier handling in AUC\n",
    "        relevant_pred_probs = relevant_pred_probs.view(-1).cpu().detach().numpy()\n",
    "        \n",
    "        # Get predictions (class with highest probability)\n",
    "        _, preds = torch.max(pred_probs, dim=1)  # Get predicted class indices\n",
    "        \n",
    "        # Accumulate predictions and labels for AUC and F1 calculation\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(word_batch.cpu().numpy())\n",
    "        \n",
    "        # Store probabilities for AUC calculation (flatten the tensor)\n",
    "        all_pred_probs.extend(pred_probs.cpu().detach().numpy())  # Full probabilities (for all classes)\n",
    "        \n",
    "        print(pred_probs.shape, preds.shape, word_batch.shape)\n",
    "        print(len(all_preds), len(all_labels), len(all_pred_probs))\n",
    "        \n",
    "    # Debug: Check number of unique classes in y_true and y_pred\n",
    "    num_classes_true = len(np.unique(all_labels))\n",
    "    num_classes_pred = all_pred_probs[0].shape[0] if all_pred_probs else 0\n",
    "    print(f\"Number of classes in y_true: {num_classes_true}, number of classes in y_pred: {num_classes_pred}\")\n",
    "    \n",
    "    # Ensure the shapes are consistent before calculating AUC\n",
    "    #if num_classes_true != num_classes_pred:\n",
    "        #raise ValueError(f\"Mismatch between the number of classes: {num_classes_true} != {num_classes_pred}\")\n",
    "    \n",
    "    # Calculate AUC (using all_pred_probs for full distribution)\n",
    "    auc = roc_auc_score(all_labels, all_pred_probs, average='macro', multi_class='ovr')\n",
    "    \n",
    "    # Calculate F1 score (for predicted classes vs true labels)\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "    return total_loss / len(dataloader), auc, f1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Validation epoch (calculate loss, AUC, F1 score)\n",
    "def validate_epoch(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_pred_probs = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for phoneme_batch, word_batch in dataloader:\n",
    "            phoneme_batch, word_batch = phoneme_batch.to(device), word_batch.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            output = model(phoneme_batch)  # Shape: (batch_size, word_vocab_size)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(output, word_batch)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Get probabilities and predictions\n",
    "            probs = output.softmax(dim=1)\n",
    "            _, preds = torch.max(output, dim=1)\n",
    "            print(probs,preds)\n",
    "            \n",
    "            # Accumulate probabilities for AUC and labels for F1 calculation\n",
    "            all_pred_probs.extend(probs.cpu().detach().numpy())\n",
    "            all_labels.extend(word_batch.cpu().detach().numpy())\n",
    "\n",
    "    # Calculate AUC and F1 score\n",
    "    auc = roc_auc_score(all_labels, all_pred_probs, average='macro', multi_class='ovr')\n",
    "    f1 = f1_score(all_labels, np.argmax(all_pred_probs, axis=1), average='macro')\n",
    "    \n",
    "    return total_loss / len(dataloader), auc, f1\n",
    "\n",
    "\n",
    "\n",
    "# Plot AUC Curve (train vs validation)\n",
    "def plot_auc_curve(train_auc_scores, val_auc_scores):\n",
    "    plt.plot(train_auc_scores, label=\"Train AUC\")\n",
    "    plt.plot(val_auc_scores, label=\"Validation AUC\")\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('AUC')\n",
    "    plt.title('AUC Curve')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Example of model training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "phoneme_vocab_size = len(all_phonemes)  # Unique phoneme count\n",
    "word_vocab_size = len(all_words)+2  # Unique word count\n",
    "\n",
    "model = Phoneme_Word_LSTM\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Assuming train_loader and val_loader are defined\n",
    "train_losses, val_losses, train_auc_scores, val_auc_scores, train_f1_scores, val_f1_scores = train_and_validate(\n",
    "    model, train_loader, val_loader, optimizer, criterion, epochs=10\n",
    ")\n",
    "\n",
    "# Plot AUC Curve\n",
    "plot_auc_curve(train_auc_scores, val_auc_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "bafb4a48-db03-49a5-a3ab-80dc2b920a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ae9e7e-0a71-436b-9826-b8c49fa65662",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_phonemes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "772605a1-86e9-476b-ae0a-263e821b2177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape before reshaping: torch.Size([32, 2356])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[-1, 68]' is invalid for input of size 75392",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[278], line 37\u001b[0m\n\u001b[0;32m     35\u001b[0m     probabilities \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(output, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     36\u001b[0m     all_targets\u001b[38;5;241m.\u001b[39mextend(batch_targets_reshaped\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m---> 37\u001b[0m     all_probabilities\u001b[38;5;241m.\u001b[39mextend(\u001b[43mprobabilities\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Calculate training AUC\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[-1, 68]' is invalid for input of size 75392"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "import torch\n",
    "\n",
    "train_auc_scores = []\n",
    "val_auc_scores = []\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    epoch_loss = 0.0\n",
    "    all_targets = []\n",
    "    all_probabilities = []\n",
    "\n",
    "    Phoneme_Word_LSTM.train()\n",
    "    for batch_inputs, batch_targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass through the Phoneme_Word_LSTM model\n",
    "        output = Phoneme_Word_LSTM(batch_inputs)\n",
    "        print(\"Output shape before reshaping:\", output.shape)\n",
    "\n",
    "        # Reshape for loss calculation\n",
    "        output_reshaped = output.view(-1, len(all_words))\n",
    "        batch_targets_reshaped = batch_targets.view(-1)\n",
    "\n",
    "        # Calculate loss and perform backpropagation\n",
    "        loss = criterion(output_reshaped, batch_targets_reshaped)\n",
    "        epoch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Collect targets and predictions for AUC calculation\n",
    "        probabilities = torch.nn.functional.softmax(output, dim=-1)\n",
    "        all_targets.extend(batch_targets_reshaped.cpu().numpy())\n",
    "        all_probabilities.extend(probabilities.view(-1, output_dim).cpu().detach().numpy())\n",
    "\n",
    "    # Calculate training AUC\n",
    "    try:\n",
    "        train_auc = roc_auc_score(all_targets, all_probabilities, multi_class=\"ovr\", average=\"weighted\")\n",
    "    except ValueError:\n",
    "        train_auc = 0\n",
    "    train_auc_scores.append(train_auc)\n",
    "\n",
    "    # Validation phase\n",
    "    Phoneme_Word_LSTM.eval()\n",
    "    val_targets = []\n",
    "    val_probabilities = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_inputs, batch_targets in val_loader:\n",
    "            output = Phoneme_Word_LSTM(batch_inputs)\n",
    "            probabilities = torch.nn.functional.softmax(output, dim=-1)\n",
    "\n",
    "            val_targets.extend(batch_targets.view(-1).cpu().numpy())\n",
    "            val_probabilities.extend(probabilities.view(-1, output_dim).cpu().numpy())\n",
    "\n",
    "    # Calculate validation AUC\n",
    "    try:\n",
    "        val_auc = roc_auc_score(val_targets, val_probabilities, multi_class=\"ovr\", average=\"weighted\")\n",
    "    except ValueError:\n",
    "        val_auc = 0\n",
    "    val_auc_scores.append(val_auc)\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_duration = end_time - start_time\n",
    "\n",
    "    # Print epoch statistics\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss / len(train_loader):.4f}, \"\n",
    "          f\"Train AUC: {train_auc:.4f}, Val AUC: {val_auc:.4f}, Duration: {epoch_duration:.2f} seconds\")\n",
    "\n",
    "# Plot AUC Curves\n",
    "plt.plot(range(1, num_epochs + 1), train_auc_scores, label='Train AUC')\n",
    "plt.plot(range(1, num_epochs + 1), val_auc_scores, label='Validation AUC')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('AUC Score')\n",
    "plt.legend()\n",
    "plt.title('AUC Score per Epoch')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e0a37690-768e-4ee2-86c2-cb1f353ff366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of trg before reshape: torch.Size([160])\n",
      "Predicted words: ['gainedaccidentallyaccidentallyaccidentallyaccidentallyaccidentallymentionmentionhold', 'baronetaccidentallylisbonlisbonaccidentallyaccidentallyaccidentallyaccidentallymention', 'baronetaccidentallyaccidentallyaccidentallyaccidentallyaccidentallymentionholdaccidentally', 'accidentallyaccidentallyaccidentallyaccidentallyaccidentallyaccidentallyaccidentallyaccidentallymention', 'baronetaccidentallyaccidentallyaccidentallymentionmentiongainedaccidentallyaccidentally', 'accidentallyaccidentallyaccidentallyaccidentallyaccidentallyaccidentallymentionmentionaccidentally', 'accidentallyaccidentallyaccidentallyaccidentallyaccidentallyaccidentallyaccidentallyaccidentallyaccidentally', 'gainedgainedgainedaccidentallyaccidentallyaccidentallyaccidentallyaccidentallyaccidentally', 'baronetyeruiningenvoysenvoyscommitteeenvoysenvoysgained', 'baronetbaronetlisbonlisbonlisbonaccidentallyaccidentallyaccidentallydirecting', 'baronetaccidentallyaccidentallyaccidentallyaccidentallyaccidentallymentionmentiongained', 'baronetbaronetaccidentallyaccidentallyaccidentallyheatedheatedresolveaccidentally', 'baronetaccidentallyaccidentallyaccidentallyaccidentallyaccidentallymentionmentiongained', 'accidentallyaccidentallyaccidentallyaccidentallyaccidentallyaccidentallyaccidentallyaccidentallyaccidentally', 'baronetaccidentallyaccidentallyaccidentallyaccidentallyheatedmentionmentiongained', 'baronetaccidentallyaccidentallyheatedheatedaccidentallymentionmentiongained']\n"
     ]
    }
   ],
   "source": [
    "# Example batch dimensions\n",
    "batch_size = 16\n",
    "target_len = 10  # Define your target length\n",
    "\n",
    "# Example target tensor (random values)\n",
    "trg = torch.randint(1, output_dim, (batch_size * target_len,))  # Ensure it's a 1D tensor for this example\n",
    "\n",
    "# Print the shape of trg to verify\n",
    "print(f\"Shape of trg before reshape: {trg.shape}\")\n",
    "\n",
    "# Reshape trg to (batch_size, target_length)\n",
    "trg = trg.view(batch_size, target_len)\n",
    "\n",
    "# Now call the model\n",
    "output = Phoneme_Word_model(src, trg)\n",
    "\n",
    "# Proceed with the rest of the code to convert output indices to characters\n",
    "\n",
    "\n",
    "# Assuming output shape is (batch_size, target_len, output_dim)\n",
    "# You can apply softmax to get probabilities and then use argmax to get indices\n",
    "output = output.view(-1, output_dim)  # Reshape for CrossEntropyLoss\n",
    "_, predicted_indices = torch.max(output, dim=1)  # Get the indices of the max probabilities\n",
    "\n",
    "# Reshape predicted_indices back to the shape of (batch_size, target_len - 1)\n",
    "predicted_indices = predicted_indices.view(batch_size, target_len - 1)\n",
    "\n",
    "# Convert predicted indices to characters\n",
    "predicted_words = []\n",
    "for i in range(predicted_indices.size(0)):\n",
    "    word = ''.join(index_to_word[idx.item()] for idx in predicted_indices[i] if idx.item() != 0)  # Ignore padding index\n",
    "    predicted_words.append(word)\n",
    "\n",
    "# Print the predicted words\n",
    "print(\"Predicted words:\", predicted_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fe85ea69-70de-4437-a8ef-621d92673467",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_sequences(mapping, phoneme_to_index, word_to_index):\n",
    "    phoneme_seq = [phoneme_to_index[phoneme] for phoneme, _ in mapping]\n",
    "    word_seq = [word_to_index[word] for _, word in mapping]  # Map each whole word to its index\n",
    "    return phoneme_seq, word_seq\n",
    "\n",
    "# Define phoneme to index mapping\n",
    "phoneme_to_index = {phoneme: idx for idx, phoneme in enumerate(all_phonemes)}\n",
    "word_to_index = {word: idx for idx, word in enumerate(all_words)}\n",
    "\n",
    "# Initialize training data\n",
    "training_data = []\n",
    "for path, mapping in phoneme_word_mappings.items():\n",
    "    phoneme_seq, word_seq = map_to_sequences(mapping, phoneme_to_index, word_to_index)\n",
    "    training_data.append((phoneme_seq, word_seq))  # Add to training data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c4cc1e4b-9b9b-4a4d-a859-d7f1935cea15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "38c43687-12f2-4b02-ad4d-24a5211f6f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0b5a6aac-25c0-40e4-a888-3bd7f08f6bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Assuming `phoneme_sequences` and `word_sequences` are lists of sequences (each sequence is a tensor of indices)\n",
    "# Pad phoneme and word sequences in the batch to the same length\n",
    "def pad_sequences(phoneme_sequences, word_sequences, pad_token=0):\n",
    "    padded_phoneme_sequences = pad_sequence(phoneme_sequences, batch_first=True, padding_value=pad_token)\n",
    "    padded_word_sequences = pad_sequence(word_sequences, batch_first=True, padding_value=pad_token)\n",
    "    return padded_phoneme_sequences, padded_word_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f1821d0b-0bf6-4640-9acc-efec229f01a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "\n",
    "class PhonemeWordDataset(Dataset):\n",
    "    def __init__(self, phoneme_sequences, word_sequences):\n",
    "        self.phoneme_sequences = phoneme_sequences\n",
    "        self.word_sequences = word_sequences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.phoneme_sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.phoneme_sequences[idx], self.word_sequences[idx]\n",
    "\n",
    "# Custom collate function to pad sequences in each batch\n",
    "def collate_fn(batch):\n",
    "    phoneme_seqs, word_seqs = zip(*batch)\n",
    "    padded_phoneme_seqs, padded_word_seqs = pad_sequences(phoneme_seqs, word_seqs, pad_token=0)\n",
    "    return padded_phoneme_seqs, padded_word_seqs\n",
    "\n",
    "# Extract phoneme and word sequences from training data\n",
    "phoneme_sequences = [torch.tensor(seq[0]) for seq in training_data]\n",
    "word_sequences = [torch.tensor(seq[1]) for seq in training_data]\n",
    "\n",
    "# Create the full dataset\n",
    "full_dataset = PhonemeWordDataset(phoneme_sequences, word_sequences)\n",
    "\n",
    "# Define the train-test split sizes\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size\n",
    "\n",
    "# Perform the random split\n",
    "train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n",
    "\n",
    "# Create DataLoader for training and test sets\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, collate_fn=collate_fn, shuffle=True, drop_last=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, collate_fn=collate_fn, shuffle=False, drop_last=True)\n",
    "\n",
    "# Now you can use train_dataloader and test_dataloader in your training and evaluation loops.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1daf9c-65ba-4475-a710-bd54fe362e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Set up device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Move model to the device\n",
    "model = Phoneme_Word_model.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding token\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Define the number of training epochs\n",
    "num_epochs = 10  # You can set this to any number based on how long you want to train\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Initialize accumulators for metrics\n",
    "    all_true_labels = []\n",
    "    all_pred_probs = []\n",
    "    \n",
    "    for src_batch, trg_batch in train_dataloader:\n",
    "        # Move to the same device as the model\n",
    "        src_batch, trg_batch = src_batch.to(device), trg_batch.to(device)\n",
    "\n",
    "        # Forward pass through the model\n",
    "        output = model(src_batch, trg_batch)\n",
    "\n",
    "        # Flatten the predictions for loss calculation\n",
    "        output = output.contiguous().view(-1, output.shape[-1])  # Shape: (batch_size * seq_len, output_dim)\n",
    "\n",
    "        # Flatten the target sequences after removing the start token\n",
    "        trg_batch = trg_batch[:, 1:].contiguous().view(-1)  # Shape: (batch_size * seq_len,)\n",
    "\n",
    "        # Check if target indices are within the range of output classes (ignoring padding tokens)\n",
    "        trg_batch[trg_batch >= output.shape[-1]] = output.shape[-1] - 1\n",
    "\n",
    "        # Calculate loss (ignore padding tokens)\n",
    "        loss = criterion(output, trg_batch)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Convert output logits to probabilities\n",
    "        pred_probs = torch.softmax(output, dim=1)\n",
    "\n",
    "        # Convert target labels and predicted probabilities to numpy arrays\n",
    "        true_labels = trg_batch.cpu().numpy()\n",
    "        pred_probs = pred_probs.cpu().detach().numpy()\n",
    "\n",
    "        # Append the current batch's true labels and predicted probabilities\n",
    "        all_true_labels.append(true_labels)\n",
    "        print(len(all_true_labels))\n",
    "        all_pred_probs.append(pred_probs)\n",
    "        print(len(all_true_labels))\n",
    "\n",
    "    # Flatten the lists of true labels and predicted probabilities for calculating metrics\n",
    "    all_true_labels = np.concatenate(all_true_labels)\n",
    "    all_pred_probs = np.concatenate(all_pred_probs)\n",
    "\n",
    "    # Calculate F1 score (ignore padding tokens)\n",
    "    # You can calculate the F1 score on non-padding classes by using `average='macro'`\n",
    "    f1 = f1_score(all_true_labels, np.argmax(all_pred_probs, axis=1), average='macro', zero_division=1)\n",
    "    \n",
    "    # Calculate Macro AUC\n",
    "    auc = roc_auc_score(all_true_labels, all_pred_probs, average='macro', multi_class='ovr')\n",
    "\n",
    "    print(f'Epoch {epoch + 1}, Loss: {loss.item()}, F1 Score: {f1:.4f}, Macro AUC: {auc:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dce228e9-4249-4685-b969-47516f91c4a8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Only one class present in y_true. ROC AUC score is not defined in that case.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 74\u001b[0m\n\u001b[0;32m     71\u001b[0m f1 \u001b[38;5;241m=\u001b[39m f1_score(all_true_labels, np\u001b[38;5;241m.\u001b[39margmax(all_pred_probs, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmacro\u001b[39m\u001b[38;5;124m'\u001b[39m, zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# Calculate Macro AUC\u001b[39;00m\n\u001b[1;32m---> 74\u001b[0m auc \u001b[38;5;241m=\u001b[39m \u001b[43mroc_auc_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_true_labels_one_hot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_pred_probs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmacro\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43movr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, F1 Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf1\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Macro AUC: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mauc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:648\u001b[0m, in \u001b[0;36mroc_auc_score\u001b[1;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[0;32m    640\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _average_binary_score(\n\u001b[0;32m    641\u001b[0m         partial(_binary_roc_auc_score, max_fpr\u001b[38;5;241m=\u001b[39mmax_fpr),\n\u001b[0;32m    642\u001b[0m         y_true,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    645\u001b[0m         sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[0;32m    646\u001b[0m     )\n\u001b[0;32m    647\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# multilabel-indicator\u001b[39;00m\n\u001b[1;32m--> 648\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_average_binary_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_binary_roc_auc_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_fpr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_fpr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    650\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    651\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    652\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    653\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    654\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_base.py:119\u001b[0m, in \u001b[0;36m_average_binary_score\u001b[1;34m(binary_metric, y_true, y_score, average, sample_weight)\u001b[0m\n\u001b[0;32m    117\u001b[0m     y_true_c \u001b[38;5;241m=\u001b[39m y_true\u001b[38;5;241m.\u001b[39mtake([c], axis\u001b[38;5;241m=\u001b[39mnot_average_axis)\u001b[38;5;241m.\u001b[39mravel()\n\u001b[0;32m    118\u001b[0m     y_score_c \u001b[38;5;241m=\u001b[39m y_score\u001b[38;5;241m.\u001b[39mtake([c], axis\u001b[38;5;241m=\u001b[39mnot_average_axis)\u001b[38;5;241m.\u001b[39mravel()\n\u001b[1;32m--> 119\u001b[0m     score[c] \u001b[38;5;241m=\u001b[39m \u001b[43mbinary_metric\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true_c\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_score_c\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscore_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;66;03m# Average the results\u001b[39;00m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:382\u001b[0m, in \u001b[0;36m_binary_roc_auc_score\u001b[1;34m(y_true, y_score, sample_weight, max_fpr)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Binary roc auc score.\"\"\"\u001b[39;00m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(np\u001b[38;5;241m.\u001b[39munique(y_true)) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m--> 382\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    383\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly one class present in y_true. ROC AUC score \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    384\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis not defined in that case.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    385\u001b[0m     )\n\u001b[0;32m    387\u001b[0m fpr, tpr, _ \u001b[38;5;241m=\u001b[39m roc_curve(y_true, y_score, sample_weight\u001b[38;5;241m=\u001b[39msample_weight)\n\u001b[0;32m    388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_fpr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m max_fpr \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[1;31mValueError\u001b[0m: Only one class present in y_true. ROC AUC score is not defined in that case."
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# Set up device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Move model to the device\n",
    "model = Phoneme_Word_model.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding token\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Define the number of training epochs\n",
    "num_epochs = 10  # You can set this to any number based on how long you want to train\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Initialize accumulators for metrics\n",
    "    all_true_labels = []\n",
    "    all_pred_probs = []\n",
    "    \n",
    "    for src_batch, trg_batch in train_dataloader:\n",
    "        # Move to the same device as the model\n",
    "        src_batch, trg_batch = src_batch.to(device), trg_batch.to(device)\n",
    "\n",
    "        # Forward pass through the model\n",
    "        output = model(src_batch, trg_batch)\n",
    "\n",
    "        # Flatten the predictions for loss calculation\n",
    "        output = output.contiguous().view(-1, output.shape[-1])  # Shape: (batch_size * seq_len, output_dim)\n",
    "\n",
    "        # Flatten the target sequences after removing the start token\n",
    "        trg_batch = trg_batch[:, 1:].contiguous().view(-1)  # Shape: (batch_size * seq_len,)\n",
    "\n",
    "        # Check if target indices are within the range of output classes (ignoring padding tokens)\n",
    "        trg_batch[trg_batch >= output.shape[-1]] = output.shape[-1] - 1\n",
    "\n",
    "        # Calculate loss (ignore padding tokens)\n",
    "        loss = criterion(output, trg_batch)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Convert output logits to probabilities\n",
    "        pred_probs = torch.softmax(output, dim=1)\n",
    "\n",
    "        # Convert target labels and predicted probabilities to numpy arrays\n",
    "        true_labels = trg_batch.cpu().numpy()\n",
    "        pred_probs = pred_probs.cpu().detach().numpy()\n",
    "\n",
    "        # Append the current batch's true labels and predicted probabilities\n",
    "        all_true_labels.append(true_labels)\n",
    "        all_pred_probs.append(pred_probs)\n",
    "\n",
    "    # Flatten the lists of true labels and predicted probabilities for calculating metrics\n",
    "    all_true_labels = np.concatenate(all_true_labels)\n",
    "    all_pred_probs = np.concatenate(all_pred_probs)\n",
    "\n",
    "    # Ensure all_true_labels is one-hot encoded (for multi-class AUC)\n",
    "    n_classes = all_pred_probs.shape[1]\n",
    "    all_true_labels_one_hot = np.zeros((all_true_labels.shape[0], n_classes))\n",
    "    all_true_labels_one_hot[np.arange(all_true_labels.shape[0]), all_true_labels] = 1\n",
    "\n",
    "    # Calculate F1 score (ignore padding tokens)\n",
    "    f1 = f1_score(all_true_labels, np.argmax(all_pred_probs, axis=1), average='macro', zero_division=1)\n",
    "\n",
    "    # Calculate Macro AUC\n",
    "    auc = roc_auc_score(all_true_labels_one_hot, all_pred_probs, average='macro', multi_class='ovr')\n",
    "\n",
    "    print(f'Epoch {epoch + 1}, Loss: {loss.item()}, F1 Score: {f1:.4f}, Macro AUC: {auc:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace17f72-1b67-4247-a8cc-2745a5359828",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Set up device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Move model to the device\n",
    "model = Phoneme_Word_model.to(device)\n",
    "\n",
    "# Set the model to evaluation mode (important for inference)\n",
    "model.eval()\n",
    "\n",
    "# Initialize accumulators for metrics\n",
    "all_true_labels = []\n",
    "all_pred_probs = []\n",
    "\n",
    "# No gradient computation during testing\n",
    "with torch.no_grad():\n",
    "    # Iterate through the test DataLoader\n",
    "    for src_batch, trg_batch in test_dataloader:\n",
    "        # Move to the same device as the model\n",
    "        src_batch, trg_batch = src_batch.to(device), trg_batch.to(device)\n",
    "\n",
    "        # Forward pass through the model\n",
    "        output = model(src_batch, trg_batch)\n",
    "\n",
    "        # Flatten the predictions for loss calculation\n",
    "        output = output.contiguous().view(-1, output.shape[-1])  # Shape: (batch_size * seq_len, output_dim)\n",
    "\n",
    "        # Flatten the target sequences after removing the start token\n",
    "        trg_batch = trg_batch[:, 1:].contiguous().view(-1)  # Shape: (batch_size * seq_len,)\n",
    "\n",
    "        # Convert output logits to probabilities\n",
    "        pred_probs = torch.softmax(output, dim=1)\n",
    "\n",
    "        # Convert target labels and predicted probabilities to numpy arrays\n",
    "        true_labels = trg_batch.cpu().numpy()\n",
    "        pred_probs = pred_probs.cpu().detach().numpy()\n",
    "\n",
    "        # Append the current batch's true labels and predicted probabilities\n",
    "        all_true_labels.append(true_labels)\n",
    "        all_pred_probs.append(pred_probs)\n",
    "\n",
    "# Flatten the lists of true labels and predicted probabilities for calculating metrics\n",
    "all_true_labels = np.concatenate(all_true_labels)\n",
    "all_pred_probs = np.concatenate(all_pred_probs)\n",
    "\n",
    "# Calculate F1 score (ignore padding tokens)\n",
    "f1 = f1_score(all_true_labels, np.argmax(all_pred_probs, axis=1), average='macro', zero_division=1)\n",
    "\n",
    "# Calculate Macro AUC\n",
    "auc = roc_auc_score(all_true_labels, all_pred_probs, avera\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a081d21c-6826-4680-93df-41b561a641bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 7.769583702087402\n"
     ]
    }
   ],
   "source": [
    "# 1. Forward pass through MFCC to Phoneme LSTM\n",
    "batch_size=10\n",
    "sequence_length=10\n",
    "input_dim=13\n",
    "x = torch.randn(batch_size, sequence_length, input_dim)  # Example MFCC input\n",
    "output_phoneme_logits = MFCC_model(x)  # Output shape: (batch_size, seq_len, output_dim)\n",
    "\n",
    "# 2. Extract predicted phonemes by taking argmax\n",
    "predicted_phonemes = output_phoneme_logits.argmax(dim=-1)  # Shape: (batch_size, seq_len)\n",
    "\n",
    "# Ensure you have a valid input for Phoneme Encoder\n",
    "# Since the PhonemeEncoder expects phonemes as indices, we can directly use predicted_phonemes\n",
    "# 3. Prepare input for the phoneme-to-word model\n",
    "# We need to ensure `predicted_phonemes` has the appropriate shape\n",
    "src = predicted_phonemes  # This will be the input to the encoder\n",
    "\n",
    "# Prepare a target tensor for the word decoder (trg)\n",
    "# Here we assume you have a valid target tensor, which you would typically obtain from your dataset.\n",
    "# For demonstration, we'll create a random target for characters.\n",
    "target_len = 10  # Set your desired target length for word decoding\n",
    "trg = torch.randint(1, output_dim, (batch_size, target_len))  # Random target word characters\n",
    "\n",
    "# 4. Forward pass through the Phoneme-to-Word model\n",
    "output = Phoneme_Word_model(src, trg)\n",
    "\n",
    "# 5. Calculate loss (reshape output for CrossEntropyLoss)\n",
    "output = output.view(-1, output_dim)  # Reshape for loss calculation\n",
    "trg = trg[:, 1:].reshape(-1)  # Reshape target tensor (ignoring the start token)\n",
    "loss = criterion(output, trg)\n",
    "print(f\"Loss: {loss.item()}\")\n",
    "\n",
    "# 6. Backward pass\n",
    "loss.backward()\n",
    "optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48142c9-1755-4924-b8a9-227499d51c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader,random_split\n",
    "\n",
    "class MFCCToWordDataset(Dataset):\n",
    "    def __init__(self, inputs_tensor, targets_word_tensor):\n",
    "        \"\"\"\n",
    "        Custom dataset for loading MFCC data (inputs) and word labels (targets).\n",
    "        \n",
    "        Args:\n",
    "            inputs_tensor (torch.Tensor): Tensor containing MFCC features of shape (num_sequences, seq_len, num_mfcc_features).\n",
    "            targets_word_tensor (torch.Tensor): Tensor containing target word labels of shape (num_sequences, seq_len).\n",
    "        \"\"\"\n",
    "        self.inputs = inputs_tensor\n",
    "        self.targets = targets_word_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the number of samples (sequences)\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return the input (MFCC sequence) and target (word label sequence) for the given index\n",
    "        return self.inputs[idx], self.targets[idx]\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the dataset\n",
    "full_dataset = MFCCToWordDataset(inputs_tensor, targets_word_tensor)\n",
    "\n",
    "# Define split sizes (e.g., 80% for training and 20% for testing)\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n",
    "\n",
    "# Create DataLoaders for training and testing\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=10, shuffle=True,num_workers=8)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=10, shuffle=False,num_workers=8)\n",
    "\n",
    "# Now you can use `train_dataloader` for training and `test_dataloader` for evaluation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5eb709-3e03-4295-abcc-f5706aa39ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "\n",
    "num_epochs=2\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    all_targets = []\n",
    "    all_predictions = []\n",
    "    all_probabilities = []\n",
    "\n",
    "    for batch_idx, (inputs_batch, targets_batch) in enumerate(train_dataloader):\n",
    "        # 1. Forward pass through MFCC-to-Phoneme LSTM\n",
    "        outputs_phoneme = MFCC_model(inputs_batch)  # Output shape: (batch_size, seq_len, phoneme_output_dim)\n",
    "\n",
    "        # 2. Extract predicted phonemes by taking argmax\n",
    "        predicted_phonemes = outputs_phoneme.argmax(dim=-1)  # Shape: (batch_size, seq_len)\n",
    "\n",
    "        # 3. Clip `predicted_phonemes` to stay within bounds for embedding\n",
    "        predicted_phonemes = torch.clamp(predicted_phonemes, 0, output_dim - 1)\n",
    "        \n",
    "        # Prepare input for the phoneme-to-word model\n",
    "        src = predicted_phonemes  # Input to the word model\n",
    "        trg = torch.clamp(targets_batch, 0, output_dim - 1)  # Ensure trg is within valid bounds\n",
    "\n",
    "        # 4. Forward pass through the Phoneme-to-Word model\n",
    "        output_word = Phoneme_Word_model(src, trg)  # Shape: (batch_size, seq_len, output_dim)\n",
    "        \n",
    "        # 5. Calculate loss (reshape output for CrossEntropyLoss)\n",
    "        output_word = output_word.view(-1, output_dim)  # Reshape for loss calculation\n",
    "        trg = trg[:, 1:].reshape(-1)  # Reshape target tensor (ignoring the start token)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(output_word, trg)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # 6. Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 7. Get predicted classes and probabilities for metrics\n",
    "        predictions = output_word.argmax(dim=-1)  # Class predictions\n",
    "        probabilities = F.softmax(output_word, dim=-1)  # Probability scores\n",
    "\n",
    "        # Flatten and accumulate for F1 and AUC computation\n",
    "        all_targets.extend(trg.cpu().numpy())\n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "        all_probabilities.extend(probabilities.detach().cpu().numpy())\n",
    "\n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(all_targets, all_predictions, average=\"weighted\")\n",
    "    \n",
    "    # Calculate AUC (if binary or multilabel; modify as needed for multiclass)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_targets, all_probabilities, multi_class=\"ovr\", average=\"weighted\")\n",
    "    except ValueError:\n",
    "        auc = None  # AUC may not be available for some cases\n",
    "\n",
    "    # Print the average loss and metrics for the epoch\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(train_dataloader):.4f}, F1 Score: {f1:.4f}, AUC: {auc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8c94fe-9063-4aaf-a600-4f7fafe2d163",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "\n",
    "#Just to test if this is working, remove the above line and replace when doing train-test split\n",
    "# Testing loop\n",
    "with torch.no_grad():\n",
    "    running_loss = 0.0\n",
    "    all_targets = []\n",
    "    all_predictions = []\n",
    "    all_probabilities = []\n",
    "\n",
    "    for batch_idx, (inputs_batch, targets_batch) in enumerate(test_dataloader):\n",
    "        # 1. Forward pass through MFCC-to-Phoneme LSTM\n",
    "        outputs_phoneme = MFCC_model(inputs_batch)  # Output shape: (batch_size, seq_len, phoneme_output_dim)\n",
    "\n",
    "        # 2. Extract predicted phonemes by taking argmax\n",
    "        predicted_phonemes = outputs_phoneme.argmax(dim=-1)  # Shape: (batch_size, seq_len)\n",
    "\n",
    "        # 3. Clip `predicted_phonemes` to stay within bounds for embedding\n",
    "        predicted_phonemes = torch.clamp(predicted_phonemes, 0, output_dim - 1)\n",
    "\n",
    "        # Prepare input for the phoneme-to-word model\n",
    "        src = predicted_phonemes  # Input to the word model\n",
    "        trg = torch.clamp(targets_batch, 0, output_dim - 1)  # Ensure trg is within valid bounds\n",
    "\n",
    "        # 4. Forward pass through the Phoneme-to-Word model\n",
    "        output_word = Phoneme_Word_model(src, trg)  # Shape: (batch_size, seq_len, output_dim)\n",
    "        \n",
    "        # 5. Calculate loss (reshape output for CrossEntropyLoss)\n",
    "        output_word = output_word.view(-1, output_dim)  # Reshape for loss calculation\n",
    "        trg = trg[:, 1:].reshape(-1)  # Reshape target tensor (ignoring the start token)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(output_word, trg)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # 6. Get predicted classes and probabilities for metrics\n",
    "        predictions = output_word.argmax(dim=-1)  # Class predictions\n",
    "        probabilities = F.softmax(output_word, dim=-1)  # Probability scores\n",
    "\n",
    "        # Flatten and accumulate for F1 and AUC computation\n",
    "        all_targets.extend(trg.cpu().numpy())\n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "        all_probabilities.extend(probabilities.cpu().numpy())\n",
    "\n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(all_targets, all_predictions, average=\"weighted\")\n",
    "    \n",
    "    # Calculate AUC (if binary or multilabel; modify as needed for multiclass)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_targets, all_probabilities, multi_class=\"ovr\", average=\"weighted\")\n",
    "    except ValueError:\n",
    "        auc = None  # AUC may not be available for some cases\n",
    "\n",
    "    # Print the average loss and metrics\n",
    "    print(f\"Test Loss: {running_loss / len(test_dataloader):.4f}, F1 Score: {f1:.4f}, AUC: {auc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0536eadd-ec70-42ab-bcb8-f284d4f3e435",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "\n",
    "def audio_to_word(audio_path, MFCC_model, Phoneme_Word_model, word_label_encoder, output_dim, mfcc_transform):\n",
    "    \"\"\"\n",
    "    Converts an audio file to a sequence of words using a combined MFCC-to-Phoneme model \n",
    "    and Phoneme-to-Word model pipeline.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    audio_path : str\n",
    "        Path to the audio file to be processed.\n",
    "    MFCC_model : torch.nn.Module\n",
    "        Neural network model that takes MFCC features as input and outputs phoneme predictions.\n",
    "    Phoneme_Word_model : torch.nn.Module\n",
    "        Neural network model that takes phoneme indices as input and outputs word character predictions.\n",
    "    word_label_encoder : sklearn.preprocessing.LabelEncoder\n",
    "        Encoder to map word character indices to actual characters/words.\n",
    "    output_dim : int\n",
    "        The output dimension of the Phoneme-to-Word model (number of unique characters/words).\n",
    "    mfcc_transform : torchaudio.transforms.MFCC\n",
    "        A transformation to convert audio waveform to MFCC features.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    predicted_words : list of str\n",
    "        The predicted sequence of words derived from the audio input.\n",
    "\n",
    "    Workflow:\n",
    "    ---------\n",
    "    1. Load audio from `audio_path` and convert to MFCC features using `mfcc_transform`.\n",
    "    2. Perform a forward pass through the `MFCC_model` to predict phonemes.\n",
    "    3. Clip predicted phonemes to be within valid bounds for the Phoneme-to-Word model.\n",
    "    4. Pass the predicted phonemes to the `Phoneme_Word_model` along with a dummy target for autoregressive decoding.\n",
    "    5. Map the output indices to words using `word_label_encoder`.\n",
    "\n",
    "    Example:\n",
    "    --------\n",
    "    >>> mfcc_transform = torchaudio.transforms.MFCC(sample_rate=16000, n_mfcc=13)\n",
    "    >>> predicted_words = audio_to_word(\"audio.wav\", MFCC_model, Phoneme_Word_model, word_label_encoder, output_dim, mfcc_transform)\n",
    "    >>> print(predicted_words)\n",
    "    ['hello', 'world']\n",
    "    \"\"\"\n",
    "    # Step 1: Load audio and convert to MFCC\n",
    "    waveform, sample_rate = torchaudio.load(audio_path)\n",
    "    mfcc_features = mfcc_transform(waveform).transpose(1, 2)  # Shape: (batch_size=1, seq_len, mfcc_dim)\n",
    "\n",
    "    # Step 2: Forward pass through MFCC-to-Phoneme model\n",
    "    output_phoneme_logits = MFCC_model(mfcc_features)  # Shape: (1, seq_len, phoneme_output_dim)\n",
    "    predicted_phonemes = output_phoneme_logits.argmax(dim=-1)  # Shape: (1, seq_len)\n",
    "\n",
    "    # Step 3: Prepare phonemes for Phoneme-to-Word model\n",
    "    src = predicted_phonemes\n",
    "\n",
    "    # Step 4: Generate target tensor (dummy input for decoding without labels)\n",
    "    target_len = src.size(1)  # Set desired target length based on the sequence length of `src`\n",
    "    trg = torch.zeros(1, target_len, dtype=torch.long)  # Shape: (1, target_len), initialized with zeros\n",
    "\n",
    "    # Step 5: Forward pass through Phoneme-to-Word model to obtain words\n",
    "    output_word_logits = Phoneme_Word_model(src, trg)  # Shape: (1, target_len, output_dim)\n",
    "    predicted_word_indices = output_word_logits.argmax(dim=-1).squeeze(0)  # Shape: (target_len,)\n",
    "\n",
    "    # Step 6: Convert predicted indices to words\n",
    "    predicted_words = [word_label_encoder.inverse_transform([index])[0] for index in predicted_word_indices]\n",
    "\n",
    "    return predicted_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4d28b8d5-d279-49b6-927f-5d2b839093ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchaudio\\functional\\functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " np.str_('accounted'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('softened'),\n",
       " np.str_('open'),\n",
       " np.str_('rose'),\n",
       " np.str_('accounted'),\n",
       " ...]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Defining mfcc_transform\n",
    "import torchaudio\n",
    "\n",
    "# Define the MFCC transformation with appropriate settings\n",
    "mfcc_transform = torchaudio.transforms.MFCC(\n",
    "    sample_rate=16000,   # Replace with your actual sample rate if different\n",
    "    n_mfcc=13            # Number of MFCC coefficients\n",
    ")\n",
    "\n",
    "audio_to_word(\"C:/Users/siddh/Downloads/Voice-001.wav\", MFCC_model, Phoneme_Word_model, word_label_encoder, output_dim, mfcc_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0458c3c-059a-4812-966c-c31703bd9b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b7c0a2-5283-4985-8cf9-866fcbea8542",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
